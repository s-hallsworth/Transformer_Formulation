{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    import torch\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    \n",
    "    has_mps = torch.backends.mps.is_built()\n",
    "    device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "except:\n",
    "    import torch\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "    \n",
    "    has_mps = torch.backends.mps.is_built()\n",
    "    device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)\n",
    "import torch\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       time         u         x\n",
      "0  0.000000  0.250000  0.125000\n",
      "1  0.001001  0.250125  0.126064\n",
      "2  0.002002  0.250251  0.127127\n",
      "3  0.003003  0.250376  0.128191\n",
      "4  0.004004  0.250502  0.129255\n",
      "train shape:  (1800, 3)\n",
      "test shape:   (1200, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"data.csv\",sep=',', header=0,index_col=False)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace inf with NaN\n",
    "df.fillna(method='ffill', inplace=True)  # Forward fill NaNs\n",
    "\n",
    "\n",
    "num_setups = 3\n",
    "split_time = 0.6\n",
    "train_data = df[df['time'] < split_time]  \n",
    "test_data = df[df['time'] >= split_time]  \n",
    "\n",
    "print(df.head())\n",
    "print('train shape: ', train_data.shape)\n",
    "print('test shape:  ', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (3, 600, 2)\n",
      "test shape:   (3, 400, 2)\n"
     ]
    }
   ],
   "source": [
    "# Select position and control columns\n",
    "train_array = train_data[['x', 'u']]\n",
    "test_array = test_data[['x', 'u']]\n",
    "\n",
    "# Reshape test and train arrays\n",
    "train_array = train_array.to_numpy().reshape(num_setups, train_array.shape[0]//num_setups, 2) \n",
    "test_array = test_array.to_numpy().reshape(num_setups, test_array.shape[0]//num_setups, 2)\n",
    "\n",
    "print('train shape: ', train_array.shape)\n",
    "print('test shape:  ', test_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape:  (3, 599, 2)\n",
      "y train shape:  (3, 599, 2)\n",
      "x test shape:  (3, 399, 2)\n",
      "y test shape:  (3, 399, 2)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for setup in data:\n",
    "        inputs.append(setup[:sequence_length])\n",
    "        targets.append(setup[1:sequence_length+1]) # predict only u values\n",
    "\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "x_train, y_train = create_sequences(train_array,train_array.shape[1]-1)\n",
    "x_test, y_test = create_sequences(test_array, test_array.shape[1]-1)\n",
    "\n",
    "print('x train shape: ', x_train.shape)\n",
    "print('y train shape: ', y_train.shape)\n",
    "print('x test shape: ', x_test.shape)\n",
    "print('y test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape:  torch.Size([3, 599, 2])\n",
      "y train shape:  torch.Size([3, 599, 2])\n",
      "x test shape:  torch.Size([3, 399, 2])\n",
      "y test shape:  torch.Size([3, 399, 2])\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "print('x train shape: ', x_train_tensor.shape)\n",
    "print('y train shape: ', y_train_tensor.shape)\n",
    "print('x test shape: ', x_test_tensor.shape)\n",
    "print('y test shape: ', y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x0000027C17866C10>\n",
      "<torch.utils.data.dataset.TensorDataset object at 0x0000027C17866C10>\n"
     ]
    }
   ],
   "source": [
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(train_dataset)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sian_\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding for Transformer\n",
    "class DecoderPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=20):\n",
    "        super(DecoderPositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Model definition using Transformer\n",
    "class DecoderTransformerModel(nn.Module):\n",
    "    global num_timesteps\n",
    "    # attention head = 4\n",
    "    # encode layers = 2\n",
    "    # input seq of size 1 (1 token)\n",
    "    # embedding dim = 64\n",
    "    # dropout = 0.2 --> every training step each neuron has 20% chance of not contributing (to the forward pass and  being updated during backward pass)\n",
    "    def __init__(self, input_dim=num_timesteps, d_model=2, nhead=2, num_layers=1, dropout=0.2):\n",
    "        super(DecoderTransformerModel, self).__init__()\n",
    "\n",
    "        # info\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # define layers\n",
    "        #self.embedding = nn.Linear(input_dim, d_model) #(input eg [128,20] with nn.Linear(20,50) --> output [128,50])\n",
    "        self.pos_encoder = DecoderPositionalEncoding(d_model, dropout)\n",
    "        decoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=10, dropout=dropout, activation=\"relu\", layer_norm_eps=1e-05, batch_first=False, norm_first=False, bias=True, device=device)\n",
    "        self.transformer_decoder = nn.TransformerEncoder(decoder_layers, num_layers,mask_check=True)\n",
    "        self.linear = nn.Linear(d_model, 2)\n",
    "\n",
    "    def forward(self, x, decoder_mask=None):\n",
    "        #x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_decoder(x, decoder_mask)\n",
    "        x = self.linear(x[:, -1, :]) #the output from the last token of the sequence is passed through a linear layer to produce the final model output.\n",
    "        return x\n",
    "\n",
    "    def get_decoder_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one element more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        #print('maask', mask)\n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "\n",
    "        return mask\n",
    "\n",
    "decoder_model = DecoderTransformerModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sian_\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "c:\\Users\\sian_\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 599, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\sian_\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 399, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Validation Loss: 2.1074\n",
      "Epoch 2/1000, Validation Loss: 2.1187\n",
      "Epoch 3/1000, Validation Loss: 2.1296\n",
      "Epoch 4/1000, Validation Loss: 2.1403\n",
      "Epoch 5/1000, Validation Loss: 2.1510\n",
      "Epoch 6/1000, Validation Loss: 2.1563\n",
      "Epoch 7/1000, Validation Loss: 2.1617\n",
      "Epoch 8/1000, Validation Loss: 2.1673\n",
      "Epoch 9/1000, Validation Loss: 2.1728\n",
      "Epoch 10/1000, Validation Loss: 2.1756\n",
      "Epoch 11/1000, Validation Loss: 2.1783\n",
      "Epoch 12/1000, Validation Loss: 2.1810\n",
      "Epoch 13/1000, Validation Loss: 2.1837\n",
      "Epoch 14/1000, Validation Loss: 2.1850\n",
      "Epoch 15/1000, Validation Loss: 2.1864\n",
      "Epoch 16/1000, Validation Loss: 2.1877\n",
      "Epoch 17/1000, Validation Loss: 2.1891\n",
      "Epoch 18/1000, Validation Loss: 2.1899\n",
      "Epoch 19/1000, Validation Loss: 2.1906\n",
      "Epoch 20/1000, Validation Loss: 2.1913\n",
      "Stopping: Loss increasing\n"
     ]
    }
   ],
   "source": [
    "# Train the decoder model\n",
    "'''\n",
    "On CPU train time = 9 min 45s\n",
    "On google collab GPU the model is trained in a matter of seconds\n",
    "'''\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(decoder_model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "epochs = 1000\n",
    "early_stop_count = 0\n",
    "min_val_loss = float('inf')\n",
    "i = 0\n",
    "for epoch in range(epochs):\n",
    "    decoder_model.train()\n",
    "    for batch in train_dataloader:\n",
    "        x_batch, y_batch = batch\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = x_batch.size(0)\n",
    "        decoder_mask = decoder_model.get_decoder_mask(sequence_length).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = decoder_model(x_batch,decoder_mask)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    decoder_model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = decoder_model(x_batch,decoder_mask)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss <= min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "    if early_stop_count >= 20:\n",
    "        print(\"Stopping: Loss increasing\")\n",
    "        break\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1064,  1.3639]])\n",
      "tensor([[-1.1064,  1.3639]])\n",
      "tensor([[-1.1064,  1.3639]])\n",
      "[tensor([[-1.1064,  1.3639]]), tensor([[-1.1064,  1.3639]]), tensor([[-1.1064,  1.3639]])]\n",
      "[tensor([[[0.7804, 0.3576],\n",
      "         [0.7815, 0.3578],\n",
      "         [0.7826, 0.3581],\n",
      "         [0.7838, 0.3583],\n",
      "         [0.7849, 0.3586],\n",
      "         [0.7860, 0.3588],\n",
      "         [0.7872, 0.3591],\n",
      "         [0.7883, 0.3594],\n",
      "         [0.7894, 0.3596],\n",
      "         [0.7905, 0.3599],\n",
      "         [0.7917, 0.3601],\n",
      "         [0.7928, 0.3604],\n",
      "         [0.7939, 0.3606],\n",
      "         [0.7951, 0.3609],\n",
      "         [0.7962, 0.3612],\n",
      "         [0.7973, 0.3614],\n",
      "         [0.7985, 0.3617],\n",
      "         [0.7996, 0.3620],\n",
      "         [0.8007, 0.3622],\n",
      "         [0.8019, 0.3625],\n",
      "         [0.8030, 0.3627],\n",
      "         [0.8041, 0.3630],\n",
      "         [0.8053, 0.3633],\n",
      "         [0.8064, 0.3635],\n",
      "         [0.8075, 0.3638],\n",
      "         [0.8087, 0.3641],\n",
      "         [0.8098, 0.3643],\n",
      "         [0.8109, 0.3646],\n",
      "         [0.8121, 0.3649],\n",
      "         [0.8132, 0.3651],\n",
      "         [0.8143, 0.3654],\n",
      "         [0.8155, 0.3657],\n",
      "         [0.8166, 0.3659],\n",
      "         [0.8177, 0.3662],\n",
      "         [0.8189, 0.3665],\n",
      "         [0.8200, 0.3667],\n",
      "         [0.8211, 0.3670],\n",
      "         [0.8223, 0.3673],\n",
      "         [0.8234, 0.3675],\n",
      "         [0.8246, 0.3678],\n",
      "         [0.8257, 0.3681],\n",
      "         [0.8268, 0.3684],\n",
      "         [0.8280, 0.3686],\n",
      "         [0.8291, 0.3689],\n",
      "         [0.8302, 0.3692],\n",
      "         [0.8314, 0.3695],\n",
      "         [0.8325, 0.3697],\n",
      "         [0.8336, 0.3700],\n",
      "         [0.8348, 0.3703],\n",
      "         [0.8359, 0.3705],\n",
      "         [0.8371, 0.3708],\n",
      "         [0.8382, 0.3711],\n",
      "         [0.8393, 0.3714],\n",
      "         [0.8405, 0.3717],\n",
      "         [0.8416, 0.3719],\n",
      "         [0.8428, 0.3722],\n",
      "         [0.8439, 0.3725],\n",
      "         [0.8450, 0.3728],\n",
      "         [0.8462, 0.3730],\n",
      "         [0.8473, 0.3733],\n",
      "         [0.8485, 0.3736],\n",
      "         [0.8496, 0.3739],\n",
      "         [0.8507, 0.3742],\n",
      "         [0.8519, 0.3744],\n",
      "         [0.8530, 0.3747],\n",
      "         [0.8542, 0.3750],\n",
      "         [0.8553, 0.3753],\n",
      "         [0.8565, 0.3756],\n",
      "         [0.8576, 0.3758],\n",
      "         [0.8587, 0.3761],\n",
      "         [0.8599, 0.3764],\n",
      "         [0.8610, 0.3767],\n",
      "         [0.8622, 0.3770],\n",
      "         [0.8633, 0.3773],\n",
      "         [0.8645, 0.3776],\n",
      "         [0.8656, 0.3778],\n",
      "         [0.8667, 0.3781],\n",
      "         [0.8679, 0.3784],\n",
      "         [0.8690, 0.3787],\n",
      "         [0.8702, 0.3790],\n",
      "         [0.8713, 0.3793],\n",
      "         [0.8725, 0.3796],\n",
      "         [0.8736, 0.3798],\n",
      "         [0.8748, 0.3801],\n",
      "         [0.8759, 0.3804],\n",
      "         [0.8770, 0.3807],\n",
      "         [0.8782, 0.3810],\n",
      "         [0.8793, 0.3813],\n",
      "         [0.8805, 0.3816],\n",
      "         [0.8816, 0.3819],\n",
      "         [0.8828, 0.3822],\n",
      "         [0.8839, 0.3825],\n",
      "         [0.8851, 0.3828],\n",
      "         [0.8862, 0.3831],\n",
      "         [0.8874, 0.3833],\n",
      "         [0.8885, 0.3836],\n",
      "         [0.8897, 0.3839],\n",
      "         [0.8908, 0.3842],\n",
      "         [0.8920, 0.3845],\n",
      "         [0.8931, 0.3848],\n",
      "         [0.8943, 0.3851],\n",
      "         [0.8954, 0.3854],\n",
      "         [0.8966, 0.3857],\n",
      "         [0.8977, 0.3860],\n",
      "         [0.8989, 0.3863],\n",
      "         [0.9000, 0.3866],\n",
      "         [0.9012, 0.3869],\n",
      "         [0.9023, 0.3872],\n",
      "         [0.9035, 0.3875],\n",
      "         [0.9046, 0.3878],\n",
      "         [0.9058, 0.3881],\n",
      "         [0.9069, 0.3884],\n",
      "         [0.9081, 0.3887],\n",
      "         [0.9092, 0.3890],\n",
      "         [0.9104, 0.3893],\n",
      "         [0.9115, 0.3896],\n",
      "         [0.9127, 0.3899],\n",
      "         [0.9138, 0.3902],\n",
      "         [0.9150, 0.3905],\n",
      "         [0.9161, 0.3908],\n",
      "         [0.9173, 0.3912],\n",
      "         [0.9185, 0.3915],\n",
      "         [0.9196, 0.3918],\n",
      "         [0.9208, 0.3921],\n",
      "         [0.9219, 0.3924],\n",
      "         [0.9231, 0.3927],\n",
      "         [0.9242, 0.3930],\n",
      "         [0.9254, 0.3933],\n",
      "         [0.9265, 0.3936],\n",
      "         [0.9277, 0.3939],\n",
      "         [0.9289, 0.3942],\n",
      "         [0.9300, 0.3945],\n",
      "         [0.9312, 0.3949],\n",
      "         [0.9323, 0.3952],\n",
      "         [0.9335, 0.3955],\n",
      "         [0.9346, 0.3958],\n",
      "         [0.9358, 0.3961],\n",
      "         [0.9370, 0.3964],\n",
      "         [0.9381, 0.3967],\n",
      "         [0.9393, 0.3971],\n",
      "         [0.9404, 0.3974],\n",
      "         [0.9416, 0.3977],\n",
      "         [0.9427, 0.3980],\n",
      "         [0.9439, 0.3983],\n",
      "         [0.9451, 0.3986],\n",
      "         [0.9462, 0.3990],\n",
      "         [0.9474, 0.3993],\n",
      "         [0.9485, 0.3996],\n",
      "         [0.9497, 0.3999],\n",
      "         [0.9509, 0.4002],\n",
      "         [0.9520, 0.4006],\n",
      "         [0.9532, 0.4009],\n",
      "         [0.9544, 0.4012],\n",
      "         [0.9555, 0.4015],\n",
      "         [0.9567, 0.4019],\n",
      "         [0.9578, 0.4022],\n",
      "         [0.9590, 0.4025],\n",
      "         [0.9602, 0.4028],\n",
      "         [0.9613, 0.4031],\n",
      "         [0.9625, 0.4035],\n",
      "         [0.9637, 0.4038],\n",
      "         [0.9648, 0.4041],\n",
      "         [0.9660, 0.4045],\n",
      "         [0.9672, 0.4048],\n",
      "         [0.9683, 0.4051],\n",
      "         [0.9695, 0.4054],\n",
      "         [0.9707, 0.4058],\n",
      "         [0.9718, 0.4061],\n",
      "         [0.9730, 0.4064],\n",
      "         [0.9742, 0.4068],\n",
      "         [0.9753, 0.4071],\n",
      "         [0.9765, 0.4074],\n",
      "         [0.9777, 0.4078],\n",
      "         [0.9788, 0.4081],\n",
      "         [0.9800, 0.4084],\n",
      "         [0.9812, 0.4088],\n",
      "         [0.9823, 0.4091],\n",
      "         [0.9835, 0.4094],\n",
      "         [0.9847, 0.4098],\n",
      "         [0.9858, 0.4101],\n",
      "         [0.9870, 0.4104],\n",
      "         [0.9882, 0.4108],\n",
      "         [0.9893, 0.4111],\n",
      "         [0.9905, 0.4114],\n",
      "         [0.9917, 0.4118],\n",
      "         [0.9929, 0.4121],\n",
      "         [0.9940, 0.4125],\n",
      "         [0.9952, 0.4128],\n",
      "         [0.9964, 0.4132],\n",
      "         [0.9975, 0.4135],\n",
      "         [0.9987, 0.4138],\n",
      "         [0.9999, 0.4142],\n",
      "         [1.0011, 0.4145],\n",
      "         [1.0022, 0.4149],\n",
      "         [1.0034, 0.4152],\n",
      "         [1.0046, 0.4156],\n",
      "         [1.0057, 0.4159],\n",
      "         [1.0069, 0.4162],\n",
      "         [1.0081, 0.4166],\n",
      "         [1.0093, 0.4169],\n",
      "         [1.0104, 0.4173],\n",
      "         [1.0116, 0.4176],\n",
      "         [1.0128, 0.4180],\n",
      "         [1.0140, 0.4183],\n",
      "         [1.0152, 0.4187],\n",
      "         [1.0163, 0.4190],\n",
      "         [1.0175, 0.4194],\n",
      "         [1.0187, 0.4197],\n",
      "         [1.0199, 0.4201],\n",
      "         [1.0210, 0.4205],\n",
      "         [1.0222, 0.4208],\n",
      "         [1.0234, 0.4212],\n",
      "         [1.0246, 0.4215],\n",
      "         [1.0258, 0.4219],\n",
      "         [1.0269, 0.4222],\n",
      "         [1.0281, 0.4226],\n",
      "         [1.0293, 0.4229],\n",
      "         [1.0305, 0.4233],\n",
      "         [1.0317, 0.4237],\n",
      "         [1.0328, 0.4240],\n",
      "         [1.0340, 0.4244],\n",
      "         [1.0352, 0.4247],\n",
      "         [1.0364, 0.4251],\n",
      "         [1.0376, 0.4255],\n",
      "         [1.0387, 0.4258],\n",
      "         [1.0399, 0.4262],\n",
      "         [1.0411, 0.4266],\n",
      "         [1.0423, 0.4269],\n",
      "         [1.0435, 0.4273],\n",
      "         [1.0447, 0.4277],\n",
      "         [1.0458, 0.4280],\n",
      "         [1.0470, 0.4284],\n",
      "         [1.0482, 0.4288],\n",
      "         [1.0494, 0.4291],\n",
      "         [1.0506, 0.4295],\n",
      "         [1.0518, 0.4299],\n",
      "         [1.0530, 0.4302],\n",
      "         [1.0541, 0.4306],\n",
      "         [1.0553, 0.4310],\n",
      "         [1.0565, 0.4313],\n",
      "         [1.0577, 0.4317],\n",
      "         [1.0589, 0.4321],\n",
      "         [1.0601, 0.4325],\n",
      "         [1.0613, 0.4328],\n",
      "         [1.0625, 0.4332],\n",
      "         [1.0636, 0.4336],\n",
      "         [1.0648, 0.4340],\n",
      "         [1.0660, 0.4343],\n",
      "         [1.0672, 0.4347],\n",
      "         [1.0684, 0.4351],\n",
      "         [1.0696, 0.4355],\n",
      "         [1.0708, 0.4359],\n",
      "         [1.0720, 0.4362],\n",
      "         [1.0732, 0.4366],\n",
      "         [1.0744, 0.4370],\n",
      "         [1.0756, 0.4374],\n",
      "         [1.0767, 0.4378],\n",
      "         [1.0779, 0.4382],\n",
      "         [1.0791, 0.4385],\n",
      "         [1.0803, 0.4389],\n",
      "         [1.0815, 0.4393],\n",
      "         [1.0827, 0.4397],\n",
      "         [1.0839, 0.4401],\n",
      "         [1.0851, 0.4405],\n",
      "         [1.0863, 0.4409],\n",
      "         [1.0875, 0.4413],\n",
      "         [1.0887, 0.4416],\n",
      "         [1.0899, 0.4420],\n",
      "         [1.0911, 0.4424],\n",
      "         [1.0923, 0.4428],\n",
      "         [1.0935, 0.4432],\n",
      "         [1.0947, 0.4436],\n",
      "         [1.0959, 0.4440],\n",
      "         [1.0971, 0.4444],\n",
      "         [1.0983, 0.4448],\n",
      "         [1.0995, 0.4452],\n",
      "         [1.1007, 0.4456],\n",
      "         [1.1019, 0.4460],\n",
      "         [1.1031, 0.4464],\n",
      "         [1.1043, 0.4468],\n",
      "         [1.1055, 0.4472],\n",
      "         [1.1067, 0.4476],\n",
      "         [1.1079, 0.4480],\n",
      "         [1.1091, 0.4484],\n",
      "         [1.1103, 0.4488],\n",
      "         [1.1115, 0.4492],\n",
      "         [1.1127, 0.4496],\n",
      "         [1.1139, 0.4500],\n",
      "         [1.1151, 0.4504],\n",
      "         [1.1163, 0.4508],\n",
      "         [1.1175, 0.4512],\n",
      "         [1.1187, 0.4516],\n",
      "         [1.1199, 0.4520],\n",
      "         [1.1211, 0.4524],\n",
      "         [1.1223, 0.4529],\n",
      "         [1.1235, 0.4533],\n",
      "         [1.1247, 0.4537],\n",
      "         [1.1259, 0.4541],\n",
      "         [1.1272, 0.4545],\n",
      "         [1.1284, 0.4549],\n",
      "         [1.1296, 0.4553],\n",
      "         [1.1308, 0.4557],\n",
      "         [1.1320, 0.4562],\n",
      "         [1.1332, 0.4566],\n",
      "         [1.1344, 0.4570],\n",
      "         [1.1356, 0.4574],\n",
      "         [1.1368, 0.4578],\n",
      "         [1.1380, 0.4583],\n",
      "         [1.1392, 0.4587],\n",
      "         [1.1405, 0.4591],\n",
      "         [1.1417, 0.4595],\n",
      "         [1.1429, 0.4599],\n",
      "         [1.1441, 0.4604],\n",
      "         [1.1453, 0.4608],\n",
      "         [1.1465, 0.4612],\n",
      "         [1.1477, 0.4616],\n",
      "         [1.1490, 0.4621],\n",
      "         [1.1502, 0.4625],\n",
      "         [1.1514, 0.4629],\n",
      "         [1.1526, 0.4634],\n",
      "         [1.1538, 0.4638],\n",
      "         [1.1550, 0.4642],\n",
      "         [1.1562, 0.4647],\n",
      "         [1.1575, 0.4651],\n",
      "         [1.1587, 0.4655],\n",
      "         [1.1599, 0.4660],\n",
      "         [1.1611, 0.4664],\n",
      "         [1.1623, 0.4668],\n",
      "         [1.1636, 0.4673],\n",
      "         [1.1648, 0.4677],\n",
      "         [1.1660, 0.4681],\n",
      "         [1.1672, 0.4686],\n",
      "         [1.1684, 0.4690],\n",
      "         [1.1697, 0.4695],\n",
      "         [1.1709, 0.4699],\n",
      "         [1.1721, 0.4703],\n",
      "         [1.1733, 0.4708],\n",
      "         [1.1746, 0.4712],\n",
      "         [1.1758, 0.4717],\n",
      "         [1.1770, 0.4721],\n",
      "         [1.1782, 0.4726],\n",
      "         [1.1794, 0.4730],\n",
      "         [1.1807, 0.4735],\n",
      "         [1.1819, 0.4739],\n",
      "         [1.1831, 0.4744],\n",
      "         [1.1844, 0.4748],\n",
      "         [1.1856, 0.4753],\n",
      "         [1.1868, 0.4757],\n",
      "         [1.1880, 0.4762],\n",
      "         [1.1893, 0.4766],\n",
      "         [1.1905, 0.4771],\n",
      "         [1.1917, 0.4775],\n",
      "         [1.1929, 0.4780],\n",
      "         [1.1942, 0.4784],\n",
      "         [1.1954, 0.4789],\n",
      "         [1.1966, 0.4794],\n",
      "         [1.1979, 0.4798],\n",
      "         [1.1991, 0.4803],\n",
      "         [1.2003, 0.4808],\n",
      "         [1.2016, 0.4812],\n",
      "         [1.2028, 0.4817],\n",
      "         [1.2040, 0.4821],\n",
      "         [1.2053, 0.4826],\n",
      "         [1.2065, 0.4831],\n",
      "         [1.2077, 0.4835],\n",
      "         [1.2090, 0.4840],\n",
      "         [1.2102, 0.4845],\n",
      "         [1.2114, 0.4850],\n",
      "         [1.2127, 0.4854],\n",
      "         [1.2139, 0.4859],\n",
      "         [1.2152, 0.4864],\n",
      "         [1.2164, 0.4868],\n",
      "         [1.2176, 0.4873],\n",
      "         [1.2189, 0.4878],\n",
      "         [1.2201, 0.4883],\n",
      "         [1.2214, 0.4887],\n",
      "         [1.2226, 0.4892],\n",
      "         [1.2238, 0.4897],\n",
      "         [1.2251, 0.4902],\n",
      "         [1.2263, 0.4907],\n",
      "         [1.2276, 0.4912],\n",
      "         [1.2288, 0.4916],\n",
      "         [1.2300, 0.4921],\n",
      "         [1.2313, 0.4926],\n",
      "         [1.2325, 0.4931],\n",
      "         [1.2338, 0.4936],\n",
      "         [1.2350, 0.4941],\n",
      "         [1.2363, 0.4946],\n",
      "         [1.2375, 0.4950],\n",
      "         [1.2388, 0.4955],\n",
      "         [1.2400, 0.4960],\n",
      "         [1.2413, 0.4965],\n",
      "         [1.2425, 0.4970],\n",
      "         [1.2437, 0.4975],\n",
      "         [1.2450, 0.4980],\n",
      "         [1.2462, 0.4985],\n",
      "         [1.2475, 0.4990],\n",
      "         [1.2487, 0.4995],\n",
      "         [1.2500, 0.5000]]]), tensor([[[-1.0606,  1.6622],\n",
      "         [-1.0569,  1.6595],\n",
      "         [-1.0531,  1.6567],\n",
      "         [-1.0494,  1.6540],\n",
      "         [-1.0456,  1.6512],\n",
      "         [-1.0419,  1.6485],\n",
      "         [-1.0382,  1.6458],\n",
      "         [-1.0345,  1.6431],\n",
      "         [-1.0308,  1.6404],\n",
      "         [-1.0271,  1.6377],\n",
      "         [-1.0234,  1.6350],\n",
      "         [-1.0197,  1.6324],\n",
      "         [-1.0161,  1.6297],\n",
      "         [-1.0124,  1.6270],\n",
      "         [-1.0088,  1.6244],\n",
      "         [-1.0051,  1.6218],\n",
      "         [-1.0015,  1.6191],\n",
      "         [-0.9979,  1.6165],\n",
      "         [-0.9943,  1.6139],\n",
      "         [-0.9907,  1.6113],\n",
      "         [-0.9871,  1.6087],\n",
      "         [-0.9835,  1.6061],\n",
      "         [-0.9799,  1.6035],\n",
      "         [-0.9763,  1.6010],\n",
      "         [-0.9728,  1.5984],\n",
      "         [-0.9692,  1.5958],\n",
      "         [-0.9657,  1.5933],\n",
      "         [-0.9621,  1.5908],\n",
      "         [-0.9586,  1.5882],\n",
      "         [-0.9551,  1.5857],\n",
      "         [-0.9516,  1.5832],\n",
      "         [-0.9481,  1.5807],\n",
      "         [-0.9446,  1.5782],\n",
      "         [-0.9411,  1.5757],\n",
      "         [-0.9376,  1.5732],\n",
      "         [-0.9341,  1.5708],\n",
      "         [-0.9307,  1.5683],\n",
      "         [-0.9272,  1.5658],\n",
      "         [-0.9237,  1.5634],\n",
      "         [-0.9203,  1.5609],\n",
      "         [-0.9169,  1.5585],\n",
      "         [-0.9134,  1.5561],\n",
      "         [-0.9100,  1.5537],\n",
      "         [-0.9066,  1.5512],\n",
      "         [-0.9032,  1.5488],\n",
      "         [-0.8998,  1.5464],\n",
      "         [-0.8964,  1.5440],\n",
      "         [-0.8930,  1.5417],\n",
      "         [-0.8896,  1.5393],\n",
      "         [-0.8863,  1.5369],\n",
      "         [-0.8829,  1.5346],\n",
      "         [-0.8796,  1.5322],\n",
      "         [-0.8762,  1.5299],\n",
      "         [-0.8729,  1.5275],\n",
      "         [-0.8695,  1.5252],\n",
      "         [-0.8662,  1.5229],\n",
      "         [-0.8629,  1.5205],\n",
      "         [-0.8596,  1.5182],\n",
      "         [-0.8563,  1.5159],\n",
      "         [-0.8530,  1.5136],\n",
      "         [-0.8497,  1.5113],\n",
      "         [-0.8464,  1.5091],\n",
      "         [-0.8431,  1.5068],\n",
      "         [-0.8399,  1.5045],\n",
      "         [-0.8366,  1.5023],\n",
      "         [-0.8333,  1.5000],\n",
      "         [-0.8301,  1.4978],\n",
      "         [-0.8268,  1.4955],\n",
      "         [-0.8236,  1.4933],\n",
      "         [-0.8204,  1.4910],\n",
      "         [-0.8172,  1.4888],\n",
      "         [-0.8139,  1.4866],\n",
      "         [-0.8107,  1.4844],\n",
      "         [-0.8075,  1.4822],\n",
      "         [-0.8043,  1.4800],\n",
      "         [-0.8011,  1.4778],\n",
      "         [-0.7980,  1.4756],\n",
      "         [-0.7948,  1.4735],\n",
      "         [-0.7916,  1.4713],\n",
      "         [-0.7884,  1.4691],\n",
      "         [-0.7853,  1.4670],\n",
      "         [-0.7821,  1.4648],\n",
      "         [-0.7790,  1.4627],\n",
      "         [-0.7758,  1.4605],\n",
      "         [-0.7727,  1.4584],\n",
      "         [-0.7696,  1.4563],\n",
      "         [-0.7665,  1.4541],\n",
      "         [-0.7633,  1.4520],\n",
      "         [-0.7602,  1.4499],\n",
      "         [-0.7571,  1.4478],\n",
      "         [-0.7540,  1.4457],\n",
      "         [-0.7509,  1.4436],\n",
      "         [-0.7479,  1.4416],\n",
      "         [-0.7448,  1.4395],\n",
      "         [-0.7417,  1.4374],\n",
      "         [-0.7386,  1.4353],\n",
      "         [-0.7356,  1.4333],\n",
      "         [-0.7325,  1.4312],\n",
      "         [-0.7295,  1.4292],\n",
      "         [-0.7264,  1.4271],\n",
      "         [-0.7234,  1.4251],\n",
      "         [-0.7204,  1.4231],\n",
      "         [-0.7173,  1.4211],\n",
      "         [-0.7143,  1.4190],\n",
      "         [-0.7113,  1.4170],\n",
      "         [-0.7083,  1.4150],\n",
      "         [-0.7053,  1.4130],\n",
      "         [-0.7023,  1.4110],\n",
      "         [-0.6993,  1.4090],\n",
      "         [-0.6963,  1.4070],\n",
      "         [-0.6934,  1.4051],\n",
      "         [-0.6904,  1.4031],\n",
      "         [-0.6874,  1.4011],\n",
      "         [-0.6844,  1.3992],\n",
      "         [-0.6815,  1.3972],\n",
      "         [-0.6785,  1.3953],\n",
      "         [-0.6756,  1.3933],\n",
      "         [-0.6726,  1.3914],\n",
      "         [-0.6697,  1.3894],\n",
      "         [-0.6668,  1.3875],\n",
      "         [-0.6639,  1.3856],\n",
      "         [-0.6609,  1.3837],\n",
      "         [-0.6580,  1.3817],\n",
      "         [-0.6551,  1.3798],\n",
      "         [-0.6522,  1.3779],\n",
      "         [-0.6493,  1.3760],\n",
      "         [-0.6464,  1.3741],\n",
      "         [-0.6435,  1.3723],\n",
      "         [-0.6406,  1.3704],\n",
      "         [-0.6378,  1.3685],\n",
      "         [-0.6349,  1.3666],\n",
      "         [-0.6320,  1.3648],\n",
      "         [-0.6292,  1.3629],\n",
      "         [-0.6263,  1.3610],\n",
      "         [-0.6234,  1.3592],\n",
      "         [-0.6206,  1.3573],\n",
      "         [-0.6178,  1.3555],\n",
      "         [-0.6149,  1.3537],\n",
      "         [-0.6121,  1.3518],\n",
      "         [-0.6093,  1.3500],\n",
      "         [-0.6064,  1.3482],\n",
      "         [-0.6036,  1.3464],\n",
      "         [-0.6008,  1.3445],\n",
      "         [-0.5980,  1.3427],\n",
      "         [-0.5952,  1.3409],\n",
      "         [-0.5924,  1.3391],\n",
      "         [-0.5896,  1.3373],\n",
      "         [-0.5868,  1.3356],\n",
      "         [-0.5840,  1.3338],\n",
      "         [-0.5812,  1.3320],\n",
      "         [-0.5785,  1.3302],\n",
      "         [-0.5757,  1.3285],\n",
      "         [-0.5729,  1.3267],\n",
      "         [-0.5702,  1.3249],\n",
      "         [-0.5674,  1.3232],\n",
      "         [-0.5647,  1.3214],\n",
      "         [-0.5619,  1.3197],\n",
      "         [-0.5592,  1.3179],\n",
      "         [-0.5564,  1.3162],\n",
      "         [-0.5537,  1.3145],\n",
      "         [-0.5510,  1.3127],\n",
      "         [-0.5483,  1.3110],\n",
      "         [-0.5455,  1.3093],\n",
      "         [-0.5428,  1.3076],\n",
      "         [-0.5401,  1.3059],\n",
      "         [-0.5374,  1.3042],\n",
      "         [-0.5347,  1.3025],\n",
      "         [-0.5320,  1.3008],\n",
      "         [-0.5293,  1.2991],\n",
      "         [-0.5266,  1.2974],\n",
      "         [-0.5239,  1.2957],\n",
      "         [-0.5213,  1.2940],\n",
      "         [-0.5186,  1.2924],\n",
      "         [-0.5159,  1.2907],\n",
      "         [-0.5133,  1.2890],\n",
      "         [-0.5106,  1.2874],\n",
      "         [-0.5079,  1.2857],\n",
      "         [-0.5053,  1.2841],\n",
      "         [-0.5026,  1.2824],\n",
      "         [-0.5000,  1.2808],\n",
      "         [-0.4973,  1.2791],\n",
      "         [-0.4947,  1.2775],\n",
      "         [-0.4921,  1.2759],\n",
      "         [-0.4894,  1.2742],\n",
      "         [-0.4868,  1.2726],\n",
      "         [-0.4842,  1.2710],\n",
      "         [-0.4816,  1.2694],\n",
      "         [-0.4790,  1.2678],\n",
      "         [-0.4764,  1.2662],\n",
      "         [-0.4738,  1.2646],\n",
      "         [-0.4712,  1.2630],\n",
      "         [-0.4686,  1.2614],\n",
      "         [-0.4660,  1.2598],\n",
      "         [-0.4634,  1.2582],\n",
      "         [-0.4608,  1.2566],\n",
      "         [-0.4582,  1.2550],\n",
      "         [-0.4557,  1.2535],\n",
      "         [-0.4531,  1.2519],\n",
      "         [-0.4505,  1.2503],\n",
      "         [-0.4479,  1.2487],\n",
      "         [-0.4454,  1.2472],\n",
      "         [-0.4428,  1.2456],\n",
      "         [-0.4403,  1.2441],\n",
      "         [-0.4377,  1.2425],\n",
      "         [-0.4352,  1.2410],\n",
      "         [-0.4326,  1.2395],\n",
      "         [-0.4301,  1.2379],\n",
      "         [-0.4276,  1.2364],\n",
      "         [-0.4250,  1.2349],\n",
      "         [-0.4225,  1.2333],\n",
      "         [-0.4200,  1.2318],\n",
      "         [-0.4175,  1.2303],\n",
      "         [-0.4150,  1.2288],\n",
      "         [-0.4125,  1.2273],\n",
      "         [-0.4100,  1.2258],\n",
      "         [-0.4074,  1.2243],\n",
      "         [-0.4049,  1.2228],\n",
      "         [-0.4025,  1.2213],\n",
      "         [-0.4000,  1.2198],\n",
      "         [-0.3975,  1.2183],\n",
      "         [-0.3950,  1.2168],\n",
      "         [-0.3925,  1.2153],\n",
      "         [-0.3900,  1.2139],\n",
      "         [-0.3876,  1.2124],\n",
      "         [-0.3851,  1.2109],\n",
      "         [-0.3826,  1.2094],\n",
      "         [-0.3802,  1.2080],\n",
      "         [-0.3777,  1.2065],\n",
      "         [-0.3752,  1.2051],\n",
      "         [-0.3728,  1.2036],\n",
      "         [-0.3703,  1.2022],\n",
      "         [-0.3679,  1.2007],\n",
      "         [-0.3654,  1.1993],\n",
      "         [-0.3630,  1.1978],\n",
      "         [-0.3606,  1.1964],\n",
      "         [-0.3581,  1.1950],\n",
      "         [-0.3557,  1.1935],\n",
      "         [-0.3533,  1.1921],\n",
      "         [-0.3509,  1.1907],\n",
      "         [-0.3484,  1.1893],\n",
      "         [-0.3460,  1.1879],\n",
      "         [-0.3436,  1.1865],\n",
      "         [-0.3412,  1.1851],\n",
      "         [-0.3388,  1.1836],\n",
      "         [-0.3364,  1.1822],\n",
      "         [-0.3340,  1.1809],\n",
      "         [-0.3316,  1.1795],\n",
      "         [-0.3292,  1.1781],\n",
      "         [-0.3268,  1.1767],\n",
      "         [-0.3244,  1.1753],\n",
      "         [-0.3221,  1.1739],\n",
      "         [-0.3197,  1.1725],\n",
      "         [-0.3173,  1.1712],\n",
      "         [-0.3149,  1.1698],\n",
      "         [-0.3126,  1.1684],\n",
      "         [-0.3102,  1.1671],\n",
      "         [-0.3078,  1.1657],\n",
      "         [-0.3055,  1.1643],\n",
      "         [-0.3031,  1.1630],\n",
      "         [-0.3008,  1.1616],\n",
      "         [-0.2984,  1.1603],\n",
      "         [-0.2961,  1.1589],\n",
      "         [-0.2937,  1.1576],\n",
      "         [-0.2914,  1.1562],\n",
      "         [-0.2890,  1.1549],\n",
      "         [-0.2867,  1.1536],\n",
      "         [-0.2844,  1.1522],\n",
      "         [-0.2821,  1.1509],\n",
      "         [-0.2797,  1.1496],\n",
      "         [-0.2774,  1.1483],\n",
      "         [-0.2751,  1.1470],\n",
      "         [-0.2728,  1.1456],\n",
      "         [-0.2705,  1.1443],\n",
      "         [-0.2681,  1.1430],\n",
      "         [-0.2658,  1.1417],\n",
      "         [-0.2635,  1.1404],\n",
      "         [-0.2612,  1.1391],\n",
      "         [-0.2589,  1.1378],\n",
      "         [-0.2566,  1.1365],\n",
      "         [-0.2543,  1.1352],\n",
      "         [-0.2521,  1.1339],\n",
      "         [-0.2498,  1.1327],\n",
      "         [-0.2475,  1.1314],\n",
      "         [-0.2452,  1.1301],\n",
      "         [-0.2429,  1.1288],\n",
      "         [-0.2407,  1.1275],\n",
      "         [-0.2384,  1.1263],\n",
      "         [-0.2361,  1.1250],\n",
      "         [-0.2338,  1.1237],\n",
      "         [-0.2316,  1.1225],\n",
      "         [-0.2293,  1.1212],\n",
      "         [-0.2271,  1.1200],\n",
      "         [-0.2248,  1.1187],\n",
      "         [-0.2226,  1.1174],\n",
      "         [-0.2203,  1.1162],\n",
      "         [-0.2181,  1.1150],\n",
      "         [-0.2158,  1.1137],\n",
      "         [-0.2136,  1.1125],\n",
      "         [-0.2113,  1.1112],\n",
      "         [-0.2091,  1.1100],\n",
      "         [-0.2069,  1.1088],\n",
      "         [-0.2046,  1.1075],\n",
      "         [-0.2024,  1.1063],\n",
      "         [-0.2002,  1.1051],\n",
      "         [-0.1980,  1.1039],\n",
      "         [-0.1957,  1.1026],\n",
      "         [-0.1935,  1.1014],\n",
      "         [-0.1913,  1.1002],\n",
      "         [-0.1891,  1.0990],\n",
      "         [-0.1869,  1.0978],\n",
      "         [-0.1847,  1.0966],\n",
      "         [-0.1825,  1.0954],\n",
      "         [-0.1803,  1.0942],\n",
      "         [-0.1781,  1.0930],\n",
      "         [-0.1759,  1.0918],\n",
      "         [-0.1737,  1.0906],\n",
      "         [-0.1715,  1.0894],\n",
      "         [-0.1693,  1.0882],\n",
      "         [-0.1671,  1.0871],\n",
      "         [-0.1649,  1.0859],\n",
      "         [-0.1628,  1.0847],\n",
      "         [-0.1606,  1.0835],\n",
      "         [-0.1584,  1.0823],\n",
      "         [-0.1562,  1.0812],\n",
      "         [-0.1541,  1.0800],\n",
      "         [-0.1519,  1.0788],\n",
      "         [-0.1497,  1.0777],\n",
      "         [-0.1476,  1.0765],\n",
      "         [-0.1454,  1.0753],\n",
      "         [-0.1433,  1.0742],\n",
      "         [-0.1411,  1.0730],\n",
      "         [-0.1390,  1.0719],\n",
      "         [-0.1368,  1.0707],\n",
      "         [-0.1347,  1.0696],\n",
      "         [-0.1325,  1.0684],\n",
      "         [-0.1304,  1.0673],\n",
      "         [-0.1282,  1.0662],\n",
      "         [-0.1261,  1.0650],\n",
      "         [-0.1240,  1.0639],\n",
      "         [-0.1218,  1.0628],\n",
      "         [-0.1197,  1.0616],\n",
      "         [-0.1176,  1.0605],\n",
      "         [-0.1154,  1.0594],\n",
      "         [-0.1133,  1.0583],\n",
      "         [-0.1112,  1.0571],\n",
      "         [-0.1091,  1.0560],\n",
      "         [-0.1070,  1.0549],\n",
      "         [-0.1048,  1.0538],\n",
      "         [-0.1027,  1.0527],\n",
      "         [-0.1006,  1.0516],\n",
      "         [-0.0985,  1.0505],\n",
      "         [-0.0964,  1.0494],\n",
      "         [-0.0943,  1.0483],\n",
      "         [-0.0922,  1.0472],\n",
      "         [-0.0901,  1.0461],\n",
      "         [-0.0880,  1.0450],\n",
      "         [-0.0859,  1.0439],\n",
      "         [-0.0838,  1.0428],\n",
      "         [-0.0818,  1.0417],\n",
      "         [-0.0797,  1.0406],\n",
      "         [-0.0776,  1.0395],\n",
      "         [-0.0755,  1.0385],\n",
      "         [-0.0734,  1.0374],\n",
      "         [-0.0713,  1.0363],\n",
      "         [-0.0693,  1.0352],\n",
      "         [-0.0672,  1.0342],\n",
      "         [-0.0651,  1.0331],\n",
      "         [-0.0631,  1.0320],\n",
      "         [-0.0610,  1.0310],\n",
      "         [-0.0589,  1.0299],\n",
      "         [-0.0569,  1.0288],\n",
      "         [-0.0548,  1.0278],\n",
      "         [-0.0527,  1.0267],\n",
      "         [-0.0507,  1.0257],\n",
      "         [-0.0486,  1.0246],\n",
      "         [-0.0466,  1.0236],\n",
      "         [-0.0445,  1.0225],\n",
      "         [-0.0425,  1.0215],\n",
      "         [-0.0404,  1.0204],\n",
      "         [-0.0384,  1.0194],\n",
      "         [-0.0364,  1.0183],\n",
      "         [-0.0343,  1.0173],\n",
      "         [-0.0323,  1.0163],\n",
      "         [-0.0303,  1.0152],\n",
      "         [-0.0282,  1.0142],\n",
      "         [-0.0262,  1.0132],\n",
      "         [-0.0242,  1.0122],\n",
      "         [-0.0221,  1.0111],\n",
      "         [-0.0201,  1.0101],\n",
      "         [-0.0181,  1.0091],\n",
      "         [-0.0161,  1.0081],\n",
      "         [-0.0141,  1.0071],\n",
      "         [-0.0120,  1.0060],\n",
      "         [-0.0100,  1.0050],\n",
      "         [-0.0080,  1.0040],\n",
      "         [-0.0060,  1.0030],\n",
      "         [-0.0040,  1.0020],\n",
      "         [-0.0020,  1.0010],\n",
      "         [ 0.0000,  1.0000]]]), tensor([[[0.8919, 1.2032],\n",
      "         [0.8944, 1.2052],\n",
      "         [0.8968, 1.2072],\n",
      "         [0.8993, 1.2092],\n",
      "         [0.9018, 1.2112],\n",
      "         [0.9042, 1.2132],\n",
      "         [0.9067, 1.2152],\n",
      "         [0.9092, 1.2172],\n",
      "         [0.9117, 1.2192],\n",
      "         [0.9142, 1.2212],\n",
      "         [0.9167, 1.2232],\n",
      "         [0.9192, 1.2252],\n",
      "         [0.9217, 1.2272],\n",
      "         [0.9242, 1.2292],\n",
      "         [0.9267, 1.2312],\n",
      "         [0.9292, 1.2332],\n",
      "         [0.9317, 1.2352],\n",
      "         [0.9343, 1.2372],\n",
      "         [0.9368, 1.2392],\n",
      "         [0.9393, 1.2412],\n",
      "         [0.9419, 1.2432],\n",
      "         [0.9444, 1.2452],\n",
      "         [0.9470, 1.2472],\n",
      "         [0.9496, 1.2492],\n",
      "         [0.9521, 1.2513],\n",
      "         [0.9547, 1.2533],\n",
      "         [0.9573, 1.2553],\n",
      "         [0.9599, 1.2573],\n",
      "         [0.9624, 1.2593],\n",
      "         [0.9650, 1.2613],\n",
      "         [0.9676, 1.2633],\n",
      "         [0.9702, 1.2653],\n",
      "         [0.9728, 1.2673],\n",
      "         [0.9754, 1.2693],\n",
      "         [0.9781, 1.2713],\n",
      "         [0.9807, 1.2733],\n",
      "         [0.9833, 1.2753],\n",
      "         [0.9859, 1.2773],\n",
      "         [0.9886, 1.2793],\n",
      "         [0.9912, 1.2813],\n",
      "         [0.9939, 1.2833],\n",
      "         [0.9965, 1.2853],\n",
      "         [0.9992, 1.2873],\n",
      "         [1.0018, 1.2893],\n",
      "         [1.0045, 1.2913],\n",
      "         [1.0072, 1.2933],\n",
      "         [1.0099, 1.2953],\n",
      "         [1.0125, 1.2973],\n",
      "         [1.0152, 1.2993],\n",
      "         [1.0179, 1.3013],\n",
      "         [1.0206, 1.3033],\n",
      "         [1.0233, 1.3053],\n",
      "         [1.0260, 1.3073],\n",
      "         [1.0287, 1.3093],\n",
      "         [1.0315, 1.3113],\n",
      "         [1.0342, 1.3133],\n",
      "         [1.0369, 1.3153],\n",
      "         [1.0397, 1.3173],\n",
      "         [1.0424, 1.3193],\n",
      "         [1.0451, 1.3213],\n",
      "         [1.0479, 1.3233],\n",
      "         [1.0506, 1.3253],\n",
      "         [1.0534, 1.3273],\n",
      "         [1.0562, 1.3293],\n",
      "         [1.0590, 1.3313],\n",
      "         [1.0617, 1.3333],\n",
      "         [1.0645, 1.3353],\n",
      "         [1.0673, 1.3373],\n",
      "         [1.0701, 1.3393],\n",
      "         [1.0729, 1.3413],\n",
      "         [1.0757, 1.3433],\n",
      "         [1.0785, 1.3453],\n",
      "         [1.0813, 1.3473],\n",
      "         [1.0841, 1.3493],\n",
      "         [1.0870, 1.3514],\n",
      "         [1.0898, 1.3534],\n",
      "         [1.0926, 1.3554],\n",
      "         [1.0955, 1.3574],\n",
      "         [1.0983, 1.3594],\n",
      "         [1.1012, 1.3614],\n",
      "         [1.1040, 1.3634],\n",
      "         [1.1069, 1.3654],\n",
      "         [1.1098, 1.3674],\n",
      "         [1.1127, 1.3694],\n",
      "         [1.1155, 1.3714],\n",
      "         [1.1184, 1.3734],\n",
      "         [1.1213, 1.3754],\n",
      "         [1.1242, 1.3774],\n",
      "         [1.1271, 1.3794],\n",
      "         [1.1300, 1.3814],\n",
      "         [1.1329, 1.3834],\n",
      "         [1.1359, 1.3854],\n",
      "         [1.1388, 1.3874],\n",
      "         [1.1417, 1.3894],\n",
      "         [1.1446, 1.3914],\n",
      "         [1.1476, 1.3934],\n",
      "         [1.1505, 1.3954],\n",
      "         [1.1535, 1.3974],\n",
      "         [1.1564, 1.3994],\n",
      "         [1.1594, 1.4014],\n",
      "         [1.1624, 1.4034],\n",
      "         [1.1654, 1.4054],\n",
      "         [1.1683, 1.4074],\n",
      "         [1.1713, 1.4094],\n",
      "         [1.1743, 1.4114],\n",
      "         [1.1773, 1.4134],\n",
      "         [1.1803, 1.4154],\n",
      "         [1.1833, 1.4174],\n",
      "         [1.1863, 1.4194],\n",
      "         [1.1894, 1.4214],\n",
      "         [1.1924, 1.4234],\n",
      "         [1.1954, 1.4254],\n",
      "         [1.1985, 1.4274],\n",
      "         [1.2015, 1.4294],\n",
      "         [1.2045, 1.4314],\n",
      "         [1.2076, 1.4334],\n",
      "         [1.2107, 1.4354],\n",
      "         [1.2137, 1.4374],\n",
      "         [1.2168, 1.4394],\n",
      "         [1.2199, 1.4414],\n",
      "         [1.2230, 1.4434],\n",
      "         [1.2261, 1.4454],\n",
      "         [1.2291, 1.4474],\n",
      "         [1.2323, 1.4494],\n",
      "         [1.2354, 1.4515],\n",
      "         [1.2385, 1.4535],\n",
      "         [1.2416, 1.4555],\n",
      "         [1.2447, 1.4575],\n",
      "         [1.2478, 1.4595],\n",
      "         [1.2510, 1.4615],\n",
      "         [1.2541, 1.4635],\n",
      "         [1.2573, 1.4655],\n",
      "         [1.2604, 1.4675],\n",
      "         [1.2636, 1.4695],\n",
      "         [1.2667, 1.4715],\n",
      "         [1.2699, 1.4735],\n",
      "         [1.2731, 1.4755],\n",
      "         [1.2763, 1.4775],\n",
      "         [1.2795, 1.4795],\n",
      "         [1.2827, 1.4815],\n",
      "         [1.2859, 1.4835],\n",
      "         [1.2891, 1.4855],\n",
      "         [1.2923, 1.4875],\n",
      "         [1.2955, 1.4895],\n",
      "         [1.2987, 1.4915],\n",
      "         [1.3020, 1.4935],\n",
      "         [1.3052, 1.4955],\n",
      "         [1.3084, 1.4975],\n",
      "         [1.3117, 1.4995],\n",
      "         [1.3149, 1.5015],\n",
      "         [1.3182, 1.5035],\n",
      "         [1.3215, 1.5055],\n",
      "         [1.3247, 1.5075],\n",
      "         [1.3280, 1.5095],\n",
      "         [1.3313, 1.5115],\n",
      "         [1.3346, 1.5135],\n",
      "         [1.3379, 1.5155],\n",
      "         [1.3412, 1.5175],\n",
      "         [1.3445, 1.5195],\n",
      "         [1.3478, 1.5215],\n",
      "         [1.3511, 1.5235],\n",
      "         [1.3545, 1.5255],\n",
      "         [1.3578, 1.5275],\n",
      "         [1.3611, 1.5295],\n",
      "         [1.3645, 1.5315],\n",
      "         [1.3678, 1.5335],\n",
      "         [1.3712, 1.5355],\n",
      "         [1.3746, 1.5375],\n",
      "         [1.3779, 1.5395],\n",
      "         [1.3813, 1.5415],\n",
      "         [1.3847, 1.5435],\n",
      "         [1.3881, 1.5455],\n",
      "         [1.3915, 1.5475],\n",
      "         [1.3949, 1.5495],\n",
      "         [1.3983, 1.5516],\n",
      "         [1.4017, 1.5536],\n",
      "         [1.4051, 1.5556],\n",
      "         [1.4085, 1.5576],\n",
      "         [1.4120, 1.5596],\n",
      "         [1.4154, 1.5616],\n",
      "         [1.4189, 1.5636],\n",
      "         [1.4223, 1.5656],\n",
      "         [1.4258, 1.5676],\n",
      "         [1.4292, 1.5696],\n",
      "         [1.4327, 1.5716],\n",
      "         [1.4362, 1.5736],\n",
      "         [1.4397, 1.5756],\n",
      "         [1.4432, 1.5776],\n",
      "         [1.4467, 1.5796],\n",
      "         [1.4502, 1.5816],\n",
      "         [1.4537, 1.5836],\n",
      "         [1.4572, 1.5856],\n",
      "         [1.4607, 1.5876],\n",
      "         [1.4642, 1.5896],\n",
      "         [1.4678, 1.5916],\n",
      "         [1.4713, 1.5936],\n",
      "         [1.4748, 1.5956],\n",
      "         [1.4784, 1.5976],\n",
      "         [1.4820, 1.5996],\n",
      "         [1.4855, 1.6016],\n",
      "         [1.4891, 1.6036],\n",
      "         [1.4927, 1.6056],\n",
      "         [1.4963, 1.6076],\n",
      "         [1.4998, 1.6096],\n",
      "         [1.5034, 1.6116],\n",
      "         [1.5070, 1.6136],\n",
      "         [1.5107, 1.6156],\n",
      "         [1.5143, 1.6176],\n",
      "         [1.5179, 1.6196],\n",
      "         [1.5215, 1.6216],\n",
      "         [1.5252, 1.6236],\n",
      "         [1.5288, 1.6256],\n",
      "         [1.5325, 1.6276],\n",
      "         [1.5361, 1.6296],\n",
      "         [1.5398, 1.6316],\n",
      "         [1.5434, 1.6336],\n",
      "         [1.5471, 1.6356],\n",
      "         [1.5508, 1.6376],\n",
      "         [1.5545, 1.6396],\n",
      "         [1.5582, 1.6416],\n",
      "         [1.5619, 1.6436],\n",
      "         [1.5656, 1.6456],\n",
      "         [1.5693, 1.6476],\n",
      "         [1.5730, 1.6496],\n",
      "         [1.5768, 1.6517],\n",
      "         [1.5805, 1.6537],\n",
      "         [1.5842, 1.6557],\n",
      "         [1.5880, 1.6577],\n",
      "         [1.5917, 1.6597],\n",
      "         [1.5955, 1.6617],\n",
      "         [1.5993, 1.6637],\n",
      "         [1.6030, 1.6657],\n",
      "         [1.6068, 1.6677],\n",
      "         [1.6106, 1.6697],\n",
      "         [1.6144, 1.6717],\n",
      "         [1.6182, 1.6737],\n",
      "         [1.6220, 1.6757],\n",
      "         [1.6258, 1.6777],\n",
      "         [1.6297, 1.6797],\n",
      "         [1.6335, 1.6817],\n",
      "         [1.6373, 1.6837],\n",
      "         [1.6412, 1.6857],\n",
      "         [1.6450, 1.6877],\n",
      "         [1.6489, 1.6897],\n",
      "         [1.6527, 1.6917],\n",
      "         [1.6566, 1.6937],\n",
      "         [1.6605, 1.6957],\n",
      "         [1.6644, 1.6977],\n",
      "         [1.6682, 1.6997],\n",
      "         [1.6721, 1.7017],\n",
      "         [1.6760, 1.7037],\n",
      "         [1.6800, 1.7057],\n",
      "         [1.6839, 1.7077],\n",
      "         [1.6878, 1.7097],\n",
      "         [1.6917, 1.7117],\n",
      "         [1.6957, 1.7137],\n",
      "         [1.6996, 1.7157],\n",
      "         [1.7036, 1.7177],\n",
      "         [1.7075, 1.7197],\n",
      "         [1.7115, 1.7217],\n",
      "         [1.7155, 1.7237],\n",
      "         [1.7194, 1.7257],\n",
      "         [1.7234, 1.7277],\n",
      "         [1.7274, 1.7297],\n",
      "         [1.7314, 1.7317],\n",
      "         [1.7354, 1.7337],\n",
      "         [1.7394, 1.7357],\n",
      "         [1.7435, 1.7377],\n",
      "         [1.7475, 1.7397],\n",
      "         [1.7515, 1.7417],\n",
      "         [1.7556, 1.7437],\n",
      "         [1.7596, 1.7457],\n",
      "         [1.7637, 1.7477],\n",
      "         [1.7677, 1.7497],\n",
      "         [1.7718, 1.7518],\n",
      "         [1.7759, 1.7538],\n",
      "         [1.7799, 1.7558],\n",
      "         [1.7840, 1.7578],\n",
      "         [1.7881, 1.7598],\n",
      "         [1.7922, 1.7618],\n",
      "         [1.7964, 1.7638],\n",
      "         [1.8005, 1.7658],\n",
      "         [1.8046, 1.7678],\n",
      "         [1.8087, 1.7698],\n",
      "         [1.8129, 1.7718],\n",
      "         [1.8170, 1.7738],\n",
      "         [1.8212, 1.7758],\n",
      "         [1.8253, 1.7778],\n",
      "         [1.8295, 1.7798],\n",
      "         [1.8337, 1.7818],\n",
      "         [1.8379, 1.7838],\n",
      "         [1.8420, 1.7858],\n",
      "         [1.8462, 1.7878],\n",
      "         [1.8504, 1.7898],\n",
      "         [1.8547, 1.7918],\n",
      "         [1.8589, 1.7938],\n",
      "         [1.8631, 1.7958],\n",
      "         [1.8673, 1.7978],\n",
      "         [1.8716, 1.7998],\n",
      "         [1.8758, 1.8018],\n",
      "         [1.8801, 1.8038],\n",
      "         [1.8843, 1.8058],\n",
      "         [1.8886, 1.8078],\n",
      "         [1.8929, 1.8098],\n",
      "         [1.8972, 1.8118],\n",
      "         [1.9015, 1.8138],\n",
      "         [1.9058, 1.8158],\n",
      "         [1.9101, 1.8178],\n",
      "         [1.9144, 1.8198],\n",
      "         [1.9187, 1.8218],\n",
      "         [1.9230, 1.8238],\n",
      "         [1.9274, 1.8258],\n",
      "         [1.9317, 1.8278],\n",
      "         [1.9360, 1.8298],\n",
      "         [1.9404, 1.8318],\n",
      "         [1.9448, 1.8338],\n",
      "         [1.9491, 1.8358],\n",
      "         [1.9535, 1.8378],\n",
      "         [1.9579, 1.8398],\n",
      "         [1.9623, 1.8418],\n",
      "         [1.9667, 1.8438],\n",
      "         [1.9711, 1.8458],\n",
      "         [1.9755, 1.8478],\n",
      "         [1.9799, 1.8498],\n",
      "         [1.9844, 1.8519],\n",
      "         [1.9888, 1.8539],\n",
      "         [1.9933, 1.8559],\n",
      "         [1.9977, 1.8579],\n",
      "         [2.0022, 1.8599],\n",
      "         [2.0066, 1.8619],\n",
      "         [2.0111, 1.8639],\n",
      "         [2.0156, 1.8659],\n",
      "         [2.0201, 1.8679],\n",
      "         [2.0246, 1.8699],\n",
      "         [2.0291, 1.8719],\n",
      "         [2.0336, 1.8739],\n",
      "         [2.0381, 1.8759],\n",
      "         [2.0426, 1.8779],\n",
      "         [2.0472, 1.8799],\n",
      "         [2.0517, 1.8819],\n",
      "         [2.0563, 1.8839],\n",
      "         [2.0608, 1.8859],\n",
      "         [2.0654, 1.8879],\n",
      "         [2.0700, 1.8899],\n",
      "         [2.0745, 1.8919],\n",
      "         [2.0791, 1.8939],\n",
      "         [2.0837, 1.8959],\n",
      "         [2.0883, 1.8979],\n",
      "         [2.0929, 1.8999],\n",
      "         [2.0976, 1.9019],\n",
      "         [2.1022, 1.9039],\n",
      "         [2.1068, 1.9059],\n",
      "         [2.1115, 1.9079],\n",
      "         [2.1161, 1.9099],\n",
      "         [2.1208, 1.9119],\n",
      "         [2.1254, 1.9139],\n",
      "         [2.1301, 1.9159],\n",
      "         [2.1348, 1.9179],\n",
      "         [2.1395, 1.9199],\n",
      "         [2.1442, 1.9219],\n",
      "         [2.1489, 1.9239],\n",
      "         [2.1536, 1.9259],\n",
      "         [2.1583, 1.9279],\n",
      "         [2.1630, 1.9299],\n",
      "         [2.1677, 1.9319],\n",
      "         [2.1725, 1.9339],\n",
      "         [2.1772, 1.9359],\n",
      "         [2.1820, 1.9379],\n",
      "         [2.1868, 1.9399],\n",
      "         [2.1915, 1.9419],\n",
      "         [2.1963, 1.9439],\n",
      "         [2.2011, 1.9459],\n",
      "         [2.2059, 1.9479],\n",
      "         [2.2107, 1.9499],\n",
      "         [2.2155, 1.9520],\n",
      "         [2.2203, 1.9540],\n",
      "         [2.2251, 1.9560],\n",
      "         [2.2300, 1.9580],\n",
      "         [2.2348, 1.9600],\n",
      "         [2.2397, 1.9620],\n",
      "         [2.2445, 1.9640],\n",
      "         [2.2494, 1.9660],\n",
      "         [2.2543, 1.9680],\n",
      "         [2.2592, 1.9700],\n",
      "         [2.2640, 1.9720],\n",
      "         [2.2689, 1.9740],\n",
      "         [2.2738, 1.9760],\n",
      "         [2.2788, 1.9780],\n",
      "         [2.2837, 1.9800],\n",
      "         [2.2886, 1.9820],\n",
      "         [2.2935, 1.9840],\n",
      "         [2.2985, 1.9860],\n",
      "         [2.3034, 1.9880],\n",
      "         [2.3084, 1.9900],\n",
      "         [2.3134, 1.9920],\n",
      "         [2.3184, 1.9940],\n",
      "         [2.3233, 1.9960],\n",
      "         [2.3283, 1.9980],\n",
      "         [2.3333, 2.0000]]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sian_\\AppData\\Local\\Temp\\ipykernel_27948\\1173641976.py:25: UserWarning: Using a target size (torch.Size([3, 399, 2])) that is different to the input size (torch.Size([3, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mse = mse_loss(all_predictions, all_targets)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (399) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [193]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m all_targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_targets, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Calculate MSE and then take the square root to get RMSE\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m mse \u001b[38;5;241m=\u001b[39m \u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mse\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sian_\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3338\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3336\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3338\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Users\\sian_\\anaconda3\\lib\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (399) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Evaluation of decoder model\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "decoder_model.eval()\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        x_batch, y_batch = batch\n",
    "        x_batch = x_batch.to(device)\n",
    "        output = decoder_model(x_batch)\n",
    "        print(output)\n",
    "        # Store predictions and targets\n",
    "        all_predictions.append(output)\n",
    "        all_targets.append(y_batch)\n",
    "\n",
    "print(all_predictions)\n",
    "print(all_targets)\n",
    "# Concatenate all batches\n",
    "all_predictions = torch.cat(all_predictions, dim=0)\n",
    "all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "# Calculate MSE and then take the square root to get RMSE\n",
    "mse = mse_loss(all_predictions, all_targets)\n",
    "rmse = np.sqrt(mse.item())\n",
    "\n",
    "print(f'Test RMSE: {rmse}')\n",
    "print(all_predictions.shape)\n",
    "print(all_targets.shape)\n",
    "\n",
    "#RMSE: The root mean square error (RMSE) measures the average difference between a statistical model’s predicted values and the actual values. \n",
    "# Mathematically, it is the standard deviation of the residuals. It is a non-standardized (0->inf) metric of a model's goodness of fit. \n",
    "# It uses the units of the dependent variable and gives a direct assessment of prediction precision. It is sensitive to outliers.\n",
    "# https://statisticsbyjim.com/regression/root-mean-square-error-rmse/\n",
    "\n",
    "#R-squared: A standardized (0->1) goodness of fit metric which is unitless. It is sensitive to outliers (see adjusted R-squared).\n",
    "#R-squared is the percentage of the response variable variation that is explained by a linear model (how close the data are to the fitted regression line).\n",
    "# R-squared = 1 - (SSR / SST)\n",
    "# The sum squared regression (SSR) is the sum of the squared differences between the predicted values and the actual values.\n",
    "# The total sum of squares (SST) represents the sum of the squares of the differences between each actual value and the overall mean of the data set.\n",
    "\n",
    "# Check back design of experiments course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y can be no greater than 2D, but have shapes (3,) and (3, 399, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [194]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Assuming 'all_predictions' and 'all_targets' are your model's predictions and actual values respectively\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mplot_predictions_vs_actual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_predictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_targets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTest Predictions vs Actual\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Calculating and displaying RMSE and R²\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, r2_score\n",
      "Input \u001b[1;32mIn [194]\u001b[0m, in \u001b[0;36mplot_predictions_vs_actual\u001b[1;34m(predictions, actual, title)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_predictions_vs_actual\u001b[39m(predictions, actual, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredictions vs Actual\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mActual Values\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(predictions, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Values\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(title)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2810\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\axes\\_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\axes\\_base.py:507\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    510\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n",
      "\u001b[1;31mValueError\u001b[0m: x and y can be no greater than 2D, but have shapes (3,) and (3, 399, 2)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions_vs_actual(predictions, actual, title='Predictions vs Actual'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(actual, label='Actual Values', color='blue')\n",
    "    plt.plot(predictions, label='Predicted Values', color='red', linestyle='--')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Assuming 'all_predictions' and 'all_targets' are your model's predictions and actual values respectively\n",
    "plot_predictions_vs_actual(all_predictions.cpu().numpy(), all_targets.cpu().numpy(), title='Test Predictions vs Actual')\n",
    "\n",
    "# Calculating and displaying RMSE and R²\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(all_targets.cpu().numpy(), all_predictions.cpu().numpy()))\n",
    "r_squared = r2_score(all_targets.cpu().numpy(), all_predictions.cpu().numpy())\n",
    "\n",
    "print(f'Test RMSE: {rmse}')\n",
    "print(f'Test R²: {r_squared}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
