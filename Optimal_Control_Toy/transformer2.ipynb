{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME SERIES DECODER ONLY TRANSFORMER\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from transformer_components import *\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# References: \n",
    "# https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Transformer(nn.Module):\n",
    "    def __init__(self,pred_length, setups, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Decoder_Transformer, self).__init__()\n",
    "        #self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model) #REMOVE EMBEDDING LAYER\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length,setups)\n",
    "        self.decoder_layers = nn.ModuleList([OnlyDecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, tgt):\n",
    "        tgt_mask = (tgt != 0).unsqueeze(3)\n",
    "        #print('target mask',tgt_mask.shape,tgt_mask)\n",
    "        \n",
    "        seq_length = tgt.size(2)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1,2, seq_length, seq_length), diagonal=1)).bool()\n",
    "        #print(nopeak_mask)\n",
    "        #print('no peak mask',nopeak_mask.shape)\n",
    "        \n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        #print('mask',tgt_mask)\n",
    "        return tgt_mask\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        tgt_mask = self.generate_mask(tgt)\n",
    "        #print(\"tgt\",type(tgt),tgt)\n",
    "        #print('pos enc', type(self.positional_encoding(tgt)), self.positional_encoding(tgt))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(tgt))\n",
    "        \n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, tgt_mask)\n",
    "\n",
    "        print(dec_output.shape)\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "Data is generated from an optimal control problem with input var x and control var u.\n",
    "\n",
    "    dx/dt = 1 + u(t)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data processing parameters\n",
    "num_setups = 3      # number of different u functions used to generate data\n",
    "split_time = 0.2   # time to split data from a scale of 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data file\n",
    "df = pd.read_csv(\"data.csv\",sep=',', header=0,index_col=False)\n",
    "\n",
    "# Replace inf with NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)  \n",
    "\n",
    "# Forward fill NaNs (fill with last valid value)\n",
    "df.fillna(method='ffill', inplace=True)  \n",
    "\n",
    "# Split timeseries data into training and test data\n",
    "train_data = df[df['time'] < split_time]  \n",
    "test_data = df[df['time'] >= split_time]  \n",
    "\n",
    "# Print\n",
    "print('df: \\n',df.head())\n",
    "print('train shape: ', train_data.shape)\n",
    "print('test shape:  ', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select position and control columns\n",
    "train_array = train_data[['x', 'u']]\n",
    "test_array = test_data[['x', 'u']]\n",
    "test_time = test_data[['time']]\n",
    "\n",
    "# Reshape test and train arrays\n",
    "train_array = train_array.to_numpy().transpose().reshape(num_setups, 2, train_array.shape[0]//num_setups) \n",
    "test_array = test_array.to_numpy().transpose().reshape(num_setups, 2, test_array.shape[0]//num_setups)\n",
    "test_time = test_time.to_numpy().transpose().reshape(num_setups, 1, test_time.shape[0]//num_setups)\n",
    "\n",
    "# Print results\n",
    "print('train shape: ', train_array.shape)\n",
    "print('test shape:  ', test_array.shape)\n",
    "#print('train data: ',train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length, pred_length):\n",
    "    inputs = []\n",
    "    targets =[]\n",
    "    \n",
    "    for setup in data:\n",
    "        # print('setup',setup)\n",
    "        # print('input', setup[:,:sequence_length-pred_length])\n",
    "        # print('target', setup[:,pred_length:sequence_length:])\n",
    "        inputs.append(setup[:,:sequence_length-pred_length])\n",
    "        targets.append(setup[:,pred_length:sequence_length:]) # predict only u values\n",
    "\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "# Create sequences \n",
    "pred_length = 10   # e.g. pred_length=2 --> data: [1,2,3,4,5], x = [1,2,], y = [3,4,5]\n",
    "\n",
    "x_train, y_train = create_sequences(train_array,train_array.shape[2], pred_length)\n",
    "x_test, y_test = create_sequences(test_array, test_array.shape[2], pred_length)\n",
    "\n",
    "# Print results\n",
    "print('x train shape: ', x_train.shape)\n",
    "print('y train shape: ', y_train.shape)\n",
    "print('x test shape: ', x_test.shape)\n",
    "print('y test shape: ', y_test.shape)\n",
    "print('x_train: ', x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test and train data to tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "#Print\n",
    "print('x train shape: ', x_train_tensor.shape) #shape: num setups, num inputs, length sequence\n",
    "print('y train shape: ', y_train_tensor.shape)\n",
    "print('x test shape: ', x_test_tensor.shape)\n",
    "print('y test shape: ', y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Transformer Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformer size\n",
    "d_model = 2                             # dimension of data 2 --> x,u\n",
    "num_heads = 2                           # number of attention heads for multihead attention\n",
    "num_layers = 1                          # number of decoder layers\n",
    "d_ff = 5                               # size of feed forward neural network\n",
    "\n",
    "# define parameters\n",
    "max_seq_length = max(train_array.shape[2],test_array.shape[2])   # maximum sequence length \n",
    "dropout = 0.1                           # dropout\n",
    "setups = train_array.shape[0]           # number of setups\n",
    "tgt_vocab_size = 5                     # for embedding but embedding not used\n",
    "\n",
    "# create transformer\n",
    "transformer = Decoder_Transformer(pred_length, setups, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "criterion = nn.MSELoss() #loss criteria\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "EPOCH = 2\n",
    "STOP_EARLY = True #false --> no early stopping\n",
    "stop_count = 200   # if the loss increases stop_count times in a row, stop early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "transformer.train()\n",
    "\n",
    "early_stop_count = 0\n",
    "min_val_loss = float('inf')\n",
    "train_losses = []\n",
    "for epoch in range(EPOCH):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for x, y in train_dataloader: #for each batch\n",
    "        optimizer.zero_grad()\n",
    "        #output = transformer(x, y[:, :-1]) #eclude last token from target\n",
    "        output = transformer(x)\n",
    "        y=y.transpose(1,2)\n",
    "        # print('x', x.shape, x)\n",
    "        # print('y', y.shape, y)\n",
    "        print('output',output.shape, output)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if loss <= min_val_loss:\n",
    "        min_val_loss = loss\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}, Avg Loss of batch: {train_loss}\")\n",
    "    if early_stop_count >= stop_count and STOP_EARLY:\n",
    "        print(\"Stopping: Loss increasing\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing loop\n",
    "transformer.eval()\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "test_losses=[]\n",
    "val_outputs = []\n",
    "with torch.no_grad():\n",
    "    test_loss=0\n",
    "    for x, y in test_dataloader:\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        val_output = transformer(x)\n",
    "        \n",
    "        val_y=y.transpose(1,2)\n",
    "        \n",
    "        loss = criterion(val_output, val_y)\n",
    "        print(f\"Validation Loss: {loss.item()}\")\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        val_outputs.append(val_y)\n",
    "    \n",
    "    test_loss /= len(test_dataloader) #mean loss of batch\n",
    "    test_losses.append(test_loss)\n",
    "print('Avg loss: ', test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation functions\n",
    "\n",
    "def plot_predictions_vs_actual(predictions, actual, title='Predictions vs Actual'):\n",
    "    \"\"\" \n",
    "    Plot the actual vs predicted results for data related to a given function of u\n",
    "\n",
    "    Args:\n",
    "        predictions (tensor): predicted values (y)\n",
    "        actual (tensor): actual values including x (inputs) and y (expected output)\n",
    "        title (str, optional): Title of plot. Defaults to 'Predictions vs Actual'.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    val=\"\"\n",
    "\n",
    "    for i in range(actual.shape[1]):\n",
    "        if i ==1:\n",
    "            val = 'u'\n",
    "        else:\n",
    "            val = 'x'\n",
    "        \n",
    "        # print('actual: ',actual[:,i])\n",
    "        # print('predicted: ',predictions[:,i])\n",
    "        plt.plot(test_time[0].squeeze(0), actual[:,i], '-o',label=f'Actual Values {val}')\n",
    "        plt.plot(test_time[0,:,actual.shape[0]-predictions.shape[0]:].squeeze(0),predictions[:,i],'--x', label=f'Predicted Values {val}')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def rmse_r2(predictions, actual, description ):\n",
    "    \"\"\"Calculates goodness of fit using:\n",
    "    (1) The root mean square error (RMSE) \n",
    "    (2) The coefficient of determination (R^2)\n",
    "\n",
    "    Args:\n",
    "        predictions: predicted values of x,u \n",
    "        actual: expected values of x,u \n",
    "        description (_type_): data description\n",
    "\n",
    "    Returns:\n",
    "        float: rmse , r_squared\n",
    "    \"\"\"\n",
    "    rmse = []\n",
    "    r_squared = []\n",
    "    for i in range(actual.shape[1]):\n",
    "        rmse.append(np.sqrt(mean_squared_error(actual[:,i], predictions[:,i])))\n",
    "        r_squared.append(r2_score(actual[:,i], predictions[:,i]))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Description': [description, description],\n",
    "        'Metric': ['Test RMSE', 'Test R²'],\n",
    "        'x': [rmse[0], r_squared[0]],\n",
    "        'u': [rmse[1], r_squared[1]]})\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "    \n",
    "    return rmse, r_squared\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics function 1\n",
    "test_array_tensor = torch.tensor(test_array, dtype=torch.float32)\n",
    "#print('full test sequence',test_array_tensor)\n",
    "\n",
    "rmse, r_squared = rmse_r2(val_outputs[0].squeeze(0),y_test_tensor.transpose(1,2)[0,:,:], 'Function 1')\n",
    "plot_predictions_vs_actual(val_outputs[0].squeeze(0),test_array_tensor.transpose(1,2)[0,:,:], f'Func 1: Predictions vs Actual (RMSE={rmse}, R^2={r_squared})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics function 2\n",
    "\n",
    "rmse_r2(val_outputs[1].squeeze(0),y_test_tensor.transpose(1,2)[1,:,:], 'Function 2')\n",
    "plot_predictions_vs_actual(val_outputs[1].squeeze(0),test_array_tensor.transpose(1,2)[1,:,:], f'Func 2: Predictions vs Actual (RMSE={rmse}, R^2={r_squared})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics function 3\n",
    "\n",
    "rmse_r2(val_outputs[2].squeeze(0),y_test_tensor.transpose(1,2)[2,:,:], 'Function 3')\n",
    "plot_predictions_vs_actual(val_outputs[2].squeeze(0),test_array_tensor.transpose(1,2)[2,:,:], f'Func 3: Predictions vs Actual (RMSE={rmse}, R^2={r_squared})') #plot for x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
