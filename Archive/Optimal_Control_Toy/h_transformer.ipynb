{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import wandb\n",
    "from transformers import TimeSeriesTransformerConfig\n",
    "from transformers import TimeSeriesTransformerForPrediction\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "# Constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# PATH = r\"/scratch/maxtheisen/Training_Pdrop\"\n",
    "# MODEL_FOLDER = \"~/MeOHReactor/models/\"\n",
    "# # Load the configuration from the JSON file\n",
    "# with open(os.path.join(\"/home/maxtheisen/MeOHReactor/\", 'config_transformer.json'), 'r') as config_file:\n",
    "#     model_config = json.load(config_file)\n",
    "\n",
    "torch.manual_seed(10)\n",
    "# Load Data\n",
    "print(\"Load data\")\n",
    "# data_files = [\"T\", \"P\", \"CO\", \"CO2\", \"H2\", \"CH4\", \"CH3OH\", \"H2O\", \"N2\"]\n",
    "# states = [pd.read_csv(f\"{PATH}/{file}.csv\", header=None) for file in data_files]\n",
    "path = \"data.csv\"\n",
    "states = pd.read_csv(path,sep=',', header=0,index_col=False)\n",
    "states.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace inf with NaN\n",
    "states.ffill(inplace=True)  # Forward fill NaNs (fill with last valid value)\n",
    "print(\"Data loaded\")\n",
    "\n",
    "# Preprocessing\n",
    "NUMBER_OF_FUNCTIONS = 3\n",
    "NUMBER_OF_POINTS = states.shape[0]//NUMBER_OF_FUNCTIONS\n",
    "PRED_LENGTH = 5\n",
    "CONTEXT_LENGTH = 10\n",
    "z = np.linspace(0, 1, NUMBER_OF_POINTS) #.reshape(-1, 1, 1) / 8.0\n",
    "# z = torch.from_numpy(z).to(DEVICE).permute(1, 0, 2)\n",
    "# states = [state.to_numpy() for state in states]\n",
    "# df_raw = np.stack(states)\n",
    "df_raw = states\n",
    "mean = df_raw.mean(axis=(1,2), keepdims=True)\n",
    "std = df_raw.std(axis=(1,2), keepdims=True)\n",
    "\n",
    "#df = ((df_raw - mean) / std).astype(np.float32)\n",
    "df = df_raw\n",
    "min = df.min(axis=(1,2), keepdims=True)\n",
    "max = df.max(axis=(1,2), keepdims=True)\n",
    "df = ((df - min) / (max - min)).astype(np.float32)\n",
    "#df = df_raw.astype(np.float32)\n",
    "df = torch.from_numpy(df).permute(1, 2, 0)[:, :NUMBER_OF_POINTS, ...]\n",
    "\n",
    "dataset = TensorDataset(df[:, 0,:].unsqueeze(1), df[:, :,:])\n",
    "\n",
    "#train_size = int(model_config['training']['train_size'] * len(dataset))\n",
    "train_size = NUMBER_OF_POINTS // 0.7\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32) #model_config['training']['batch_size'])\n",
    "val_loader = DataLoader(val_dataset, batch_size=32) #model_config['training']['batch_size'])\n",
    "\n",
    "\n",
    "def train()\n",
    "    # Model Configuration\n",
    "    config = TimeSeriesTransformerConfig(\n",
    "        prediction_length= PRED_LENGTH,  #NUMBER_OF_POINTS,\n",
    "        context_length= CONTEXT_LENGTH,#model_config['model']['context_length'],\n",
    "        embedding_dimension= 64, #model_config['model'][\"hyperparameters\"]['embedding_dim'],\n",
    "        #scaling=model_config['model']['with_scaling'],\n",
    "        lags_sequence=[0],\n",
    "        num_time_features=1,\n",
    "        input_size= NUMBER_OF_FUNCTIONS,#len(data_files),\n",
    "        num_parallel_samples=1,\n",
    "        #loss= ,#model_config['training'][\"loss\"]\n",
    "    )\n",
    "    model = TimeSeriesTransformerForPrediction(config=config).to(DEVICE)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr= 6e-4, weight_decay=10e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=0.002)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    epochs = 100\n",
    "\n",
    "    # wandb.init(project=\"MeOH-TubolarPacketBedReactor\", name=\"Time_series_model_deterministic\", config=model_config, mode=\"online\")\n",
    "\n",
    "    # # If continuing training, load model weights\n",
    "    # if model_config['training']['continue_training']:\n",
    "    #     model_weights = torch.load(\"models/model_Time_series_model_deterministic_final.pth\")\n",
    "    #     model.load_state_dict(model_weights)\n",
    "    #     model.to(DEVICE)\n",
    "\n",
    "    ## MODEL TRAINING \n",
    "    best_mape = 100\n",
    "    model_name = 'model' #model_{}_final.pth'.format(model_config[\"model\"][\"name\"])\n",
    "\n",
    "    # if model_config['training']['continue_training']:\n",
    "    #     model_weights = torch.load(MODEL_FOLDER + model_name)\n",
    "    #     model.load_state_dict(model_weights)\n",
    "    #     model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "    def plot_predictions(z, y_val, yhat):\n",
    "        titles = \" \"\n",
    "        sample = random.randint(0, y_val.shape[0] - 1)  # Ensure the index is within bounds\n",
    "        images = []\n",
    "\n",
    "        for a in range(NUMBER_OF_FUNCTIONS):\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(z[0, :, 0], y_val[sample, :, a], label=\"Ground truth\")\n",
    "            ax.plot(z[0, :, 0], yhat[sample, :, a], label=\"Prediction\")\n",
    "            ax.set_title(titles[a])\n",
    "            ax.legend()\n",
    "            \n",
    "            # Convert the figure to a wandb Image and append to the images list\n",
    "            images.append(wandb.Image(fig))\n",
    "\n",
    "            # Close the figure to free up memory\n",
    "            plt.close(fig)\n",
    "\n",
    "        return images\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        scheduler.step()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # Forward pass\n",
    "            past_time_features =  z[:, 0:1].repeat(x_batch.size(0), CONTEXT_LENGTH, 1).to(DEVICE).float()#torch.zeros_like(torch.linspace(-1, 0, CONTEXT_LENGTH).reshape(1, -1, 1).repeat(x_batch.size(0), 1, 1)).to(device)\n",
    "            future_time_features = z.repeat(x_batch.size(0), 1, 1).to(DEVICE).float() #torch.zeros_like(y_batch[..., 0]).unsqueeze(-1).to(device)\n",
    "            past_values = x_batch.repeat(1, CONTEXT_LENGTH, 1).to(DEVICE)\n",
    "            past_observed_mask = torch.zeros_like(past_values).to(DEVICE)\n",
    "            past_observed_mask[:, -1:, :] = 1\n",
    "\n",
    "            output = model(past_values = past_values, \n",
    "                        past_time_features = past_time_features,#z.repeat(y_batch.size(0), 1, 1)[:, 0, ...].unsqueeze(1), \n",
    "                        past_observed_mask = past_observed_mask,\n",
    "                        future_values = y_batch.to(DEVICE),\n",
    "                        future_time_features = future_time_features)\n",
    "            loss = output.loss\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            wandb.log({\"Training loss\": loss.item()})   # iteration loss = loss on the batch\n",
    "\n",
    "        # Validation\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vector_lv = []\n",
    "            vector_r2 = []\n",
    "            vector_mape = []\n",
    "            val_loss_2 = []\n",
    "            for x_val, y_val in val_loader:\n",
    "                # Inference on validation\n",
    "                past_time_features =  z[:, 0:1].repeat(x_val.size(0), CONTEXT_LENGTH, 1).to(DEVICE).float()#torch.zeros_like(torch.linspace(-1, 0, CONTEXT_LENGTH).reshape(1, -1, 1).repeat(x_batch.size(0), 1, 1)).to(device)\n",
    "                future_time_features = z.repeat(x_val.size(0), 1, 1).to(DEVICE).float() #torch.zeros_like(y_batch[..., 0]).unsqueeze(-1).to(device)\n",
    "                past_values = x_val.repeat(1, CONTEXT_LENGTH, 1).to(DEVICE)\n",
    "                past_observed_mask = torch.zeros_like(past_values).to(DEVICE)\n",
    "                past_observed_mask[:, -1:, :] = 1\n",
    "                output = model.generate(past_values=past_values, \n",
    "                                        past_time_features= past_time_features, \n",
    "                                        past_observed_mask = past_observed_mask,\n",
    "                                        future_time_features = future_time_features)\n",
    "                \n",
    "                #output = model.generate(past_values=x_val, past_time_features=z.repeat(y_batch.size(0), 1, 1)[:, 0, ...].unsqueeze(1), past_observed_mask = torch.ones_like(x_batch), \n",
    "                #                      future_time_features = z.repeat(y_batch.size(0), 1, 1)[:, 1:, ...])\n",
    "                yhat = output.sequences.mean(dim=1) \n",
    "\n",
    "                output = model(past_values = past_values, \n",
    "                        past_time_features = past_time_features,#z.repeat(y_batch.size(0), 1, 1)[:, 0, ...].unsqueeze(1), \n",
    "                        past_observed_mask = past_observed_mask,\n",
    "                        future_values = y_val.to(DEVICE),\n",
    "                        future_time_features = future_time_features)\n",
    "                loss = output.loss\n",
    "                val_loss_2 += [loss.cpu().detach().numpy()]\n",
    "\n",
    "\n",
    "                val_loss = loss_fn(yhat, y_val.to(DEVICE))\n",
    "                vector_lv.append(val_loss.item())\n",
    "                # R2 score on validation\n",
    "                yhat = yhat.cpu().detach().numpy()\n",
    "                y_val = y_val.cpu().detach().numpy()\n",
    "                r2 = r2_score(y_val.reshape(-1, len(data_files))[:,:-1], yhat.reshape(-1, len(data_files))[:,:-1])\n",
    "                mape = mean_absolute_percentage_error(y_val.reshape(-1, len(data_files))[:,:-1], yhat.reshape(-1, len(data_files))[:,:-1])\n",
    "                vector_r2.append(r2)\n",
    "                vector_mape.append(mape)\n",
    "\n",
    "\n",
    "            images_to_log = plot_predictions(z, y_val=y_val, yhat=yhat)\n",
    "                #        break\n",
    "            # Log validation loss and R2 score\n",
    "            #if epoch % 10 == 0:\n",
    "            #    plot_predictions(z, y_val, yhat)\n",
    "            wandb.log({\"Validation loss\": np.mean(vector_lv), # epoch loss = mean loss on the validation set at the end of the epoch\n",
    "                        \"R2 score\": np.mean(vector_r2),        # epoch R2 score = mean R2 score on the validation set at the end of the epoch\n",
    "                        \"Test MAPE score\": np.mean(vector_mape),       # epoch MAPE score = mean MAPE score on the validation set at the end of the epoch\n",
    "                        \"Epoch\": epoch,\n",
    "                        \"Prediction_example\": images_to_log})        # epoch MAPE score = mean MAPE score on the validation set at the end of the epoch\n",
    "            if np.mean(vector_mape) < best_mape: # save best checkpoint\n",
    "                torch.save(model.state_dict(), MODEL_FOLDER + model_name)\n",
    "                best_mape = np.mean(vector_mape)\n",
    "        #print(\"Val loss 2\", np.mean(val_loss_2))\n",
    "        print(\"Epoch: {} | Training loss: {} | Validation loss: {} | R2: {} | MAPE: {}\".format(epoch+1, loss.item(), val_loss.item(), np.mean(vector_r2), np.mean(vector_mape)))\n",
    "    #plot_predictions(z, y_val, yhat)\n",
    "\n",
    "    print(\"#\"*20, \"Best MAPE: {}\".format(best_mape), \"#\"*20)\n",
    "    # model_weights = torch.load(MODEL_FOLDER + model_name)\n",
    "    # best_model = TimeSeriesTransformerForPrediction(config=config)\n",
    "    # best_model.load_state_dict(model_weights)\n",
    "    # best_model.to(DEVICE)\n",
    "\n",
    "# mape_per_length = []\n",
    "# for x_val, y_val in val_loader:\n",
    "#     # Inference on validation\n",
    "#     past_time_features = torch.zeros_like(torch.linspace(-1, 0, CONTEXT_LENGTH).reshape(1, -1, 1).repeat(x_val.size(0), 1, 1)).to(DEVICE)\n",
    "#     future_time_features = torch.zeros_like(y_val[..., 0]).unsqueeze(-1).to(DEVICE)\n",
    "#     past_values = x_val.repeat(1, CONTEXT_LENGTH, 1).to(DEVICE)\n",
    "#     past_observed_mask = torch.zeros_like(past_values).to(DEVICE)\n",
    "#     past_observed_mask[:, -1:, :] = 1\n",
    "#     output = model.generate(past_values=past_observed_mask, \n",
    "#                             past_time_features= past_time_features, \n",
    "#                             past_observed_mask = past_observed_mask,\n",
    "#                             future_time_features = future_time_features)\n",
    "#     yhat = output.sequences.mean(dim=1) \n",
    "#     yhat = yhat.transpose(2,1)\n",
    "#     y_val = y_val.transpose(2,1)\n",
    "#     yhat = yhat.detach().numpy()\n",
    "#     y_val = y_val.detach().numpy()\n",
    "#     mape = mean_absolute_percentage_error(y_val.reshape(-1, y_val.shape[2]), yhat.reshape(-1, yhat.shape[2]), multioutput=\"raw_values\")\n",
    "#     mape_per_length.append(mape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
