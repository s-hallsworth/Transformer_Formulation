{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An explanation of how to set up the MINLP TNN package for use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: This file is not meant to be run but rather to give an idea of the necessary functions, where they can be found and how they can be implemented. The case studies in the example foler can be run to see this code in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyomo.environ as pyo\n",
    "import numpy as np\n",
    "import os\n",
    "import MINLP_tnn.helpers.extract_from_pretrained as extract_from_pretrained\n",
    "from gurobipy import GRB\n",
    "from gurobi_ml import add_predictor_constr\n",
    "from MINLP_tnn.helpers.print_stats import save_gurobi_results\n",
    "import MINLP_tnn.helpers.convert_pyomo as convert_pyomo\n",
    "from MINLP_tnn.helpers.GUROBI_ML_helper import get_inputs_gurobipy_FFN\n",
    "from combine_csv import combine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained TNN & Get Learnt Parameters\n",
    "\n",
    "The parsing functions have a parameter to pass input values to the TNN model. This is necessary:\n",
    "1) To determine the execution order of TNN layers by passing dummy values through the NN and monitoring how layer functions are called.\n",
    "    * Dummy values can be used as place holders. The sample inputs are a passed through the NN in a forward pass to parse the order of layer execution. In some cases the model summary does not print the layers in the order in which they are implemented by the NN.\n",
    "2) To verify that the trained TNN and formulated TNN give the same result.\n",
    "    * If you supply actual data points to the parsing functions, the \"layer_outputs_dict\" will return the output of each tnn layer due to the input values. This feature can be used to compare the trained TNNs output form each layer to the output of each layer of the formulated TNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch:\n",
    "- supports ReLU activation functions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cpu\"\n",
    "tnn_path = \" \" #TO DO: set path\n",
    "tnn_model = torch.load(tnn_path, map_location=device, weights_only=False)\n",
    "\n",
    "# PARSE ViT_tnn:\n",
    "layer_names, parameters, _, layer_outputs_dict = extract_from_pretrained.get_torchViT_learned_parameters(tnn_model, input, heads)\n",
    "        \n",
    "# PARSE OTHER PYTORCH\n",
    "layer_names, dict_transformer_params, tnn_model, [count_encoder_layers, count_decoder_layers], layer_outputs_dict = extract_from_pretrained.get_pytorch_learned_parameters(model, enc_input, dec_input, num_heads, sequence_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face's Pretrained Time Series:\n",
    "- supports ReLU activation functions\n",
    "- supports SiLU activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.time_series_transformer.configuration_time_series_transformer import TimeSeriesTransformerConfig\n",
    "from transformers.models.time_series_transformer.modeling_time_series_transformer import TimeSeriesTransformerForPrediction\n",
    "# cloned transformers from: https://github.com/s-hallsworth/transformers.git\n",
    "\n",
    "# LOAD MODEL\n",
    "device = \"cpu\"\n",
    "train_tnn_path = \" \" #TO DO: set path\n",
    "NUMBER_OF_POINTS = 8\n",
    "\n",
    "config = TimeSeriesTransformerConfig(\n",
    "        prediction_length=NUMBER_OF_POINTS,\n",
    "    )\n",
    "tnn_model = TimeSeriesTransformerForPrediction(config).to(device)\n",
    "tnn_model = torch.load(train_tnn_path, weights_only=False, map_location=torch.device('cpu'))\n",
    "tnn_model.config.prediction_length = NUMBER_OF_POINTS\n",
    "tnn_model.config.context_length=3\n",
    "tnn_model.config.embedding_dimension=60\n",
    "tnn_model.config.scaling=False\n",
    "tnn_model.config.lags_sequence=[0]\n",
    "tnn_model.config.num_time_features=1\n",
    "tnn_model.config.input_size=9\n",
    "tnn_model.config.num_parallel_samples=1\n",
    "\n",
    "# CONFIG SAMPLE INPUTS TO MATCH EXPECTED INPUT SHAPES\n",
    "hugging_face_dict = {}\n",
    "hugging_face_dict[\"past_values\"] =  past_values\n",
    "hugging_face_dict[\"past_time_features\"] = past_time_features\n",
    "hugging_face_dict[\"past_observed_mask\"] = past_observed_mask\n",
    "hugging_face_dict[\"future_time_features\"] = future_time_features\n",
    "\n",
    "src = torch.ones(1, tnn_model.config.input_size) #dummy input encoder\n",
    "tgt = torch.ones(1,  NUMBER_OF_POINTS, tnn_model.config.input_size) #dummy input decoder\n",
    "\n",
    "# GET LEARNT PARAMS:\n",
    "layer_names, parameters, _, enc_dec_count, layer_outputs_dict = extract_from_pretrained.get_hugging_learned_parameters(tnn_model, src , tgt, 2, hugging_face_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras:\n",
    "- supports ReLU activation functions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get learnt parameters\n",
    "\n",
    "tnn_model_PATH = \" \" # TO DO: fill in path\n",
    "layer_names, parameters , tnn_model = extract_from_pretrained.get_learned_parameters(tnn_model_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create MINLP_tnn instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MINLP_tnn.transformer import Transformer as TNN\n",
    "\n",
    "# DEFINE TNN HYPER-PARAMETERS\n",
    "\n",
    "# define hyper parameter list\n",
    "config_list = [encoder_sequence_length, embedding_dim, head_size , num_heads, input_fetaure dim, epsilon_layer_normalisation ]\n",
    "\n",
    "# or a path to a json with hyper parameters\n",
    "config_list = \"\\path\\to\\json.json\"\n",
    "\n",
    "\n",
    "# CREATE TRANSFORMER\n",
    "transformer = TNN.Transformer(config, pyomo_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add layers to MINLP_tnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Pytorch TNN. See transformer.py for the caveats of this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build default Pytorch TNN\n",
    "transformer.build_from_hug_torch(tnn_model, sample_enc_input, sample_dec_input, enc_bounds = None , dec_bounds = None, Transformer='pytorch', default=True, hugging_face_dict=None)\n",
    "\n",
    "# OR: Parse Pytorch TNN and add residual layers with normalisation layers\n",
    "transformer.build_from_hug_torch(tnn_model, sample_enc_input, sample_dec_input, enc_bounds = None , dec_bounds = None, Transformer='pytorch', default=False, hugging_face_dict=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or add layers individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add input variable\n",
    "input_var = transformer.add_input_var(input_var_name, dims, bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add positional encoding\n",
    "PE_var = transformer.add_pos_encoding(input_var_name:Union[pyo.Var,str], embed_var_name, b_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add embedding layer\n",
    "EMD_var = transformer.embed_input(input_var_name:Union[pyo.Var,str], embed_var_name, embed_dim_2, W_emb=None, b_emb = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add layer normalisation layer\n",
    "LN_var = transformer.add_layer_norm(input_var_name:Union[pyo.Var,str], LN_var_name, gamma= None, beta = None, eps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add attention layer: config cross attention, masked attention, max_normalised softmax\n",
    "MHA_var = transformer.add_attention(input_var_name:Union[pyo.Var,str], output_var_name, W_q, W_k, W_v, W_o, b_q = None, b_k = None, b_v = None, b_o = None, mask=False, cross_attn=False, encoder_output:Union[pyo.Var,str]=None, exp_approx=False, norm_softmax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add residual layer\n",
    "RES_var = transformer.add_residual_connection(input_1_name:Union[pyo.Var,str], input_2_name:Union[pyo.Var,str], output_var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYOMO: add FFN to model using OMLT\n",
    "from omlt.neuralnet import ReluBigMFormulation\n",
    "\n",
    "FNN_var = transformer.add_FFN_2D(input_var_name:Union[pyo.Var,str], output_var_name, nn_name, input_shape, model_parameters, bounds = (-2, 2), formulation=ReluBigMFormulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUROBI conversion of Pyomo model: add FFN to model using GurobiML (a forked version is created to support SiLU activation)\n",
    "\n",
    "# get FNN\n",
    "ffn_parameter_dict = {}\n",
    "ffn_parameter_dict[\"unique_fnn_name\"] = transformer.get_ffn(input_var_name:Union[pyo.Var,str], output_var_name, nn_name, input_shape, model_parameters)\n",
    "\n",
    "# convert Pyomo model to Gurobi\n",
    "gurobi_model, map_var, _ = convert_pyomo.to_gurobi(pyomo_model) ##--- CONVERT PYOMO MODEL TO GUROBIPY ---##\n",
    "\n",
    "# add FNN to Gurobi using GurobiML\n",
    "for key, value in ffn_parameter_dict.items():\n",
    "    nn, input_nn, output_nn = value\n",
    "    input, output = get_inputs_gurobipy_FFN(input_nn, output_nn, map_var)\n",
    "    pred_constr = add_predictor_constr(gurobi_model, nn, input, output)\n",
    "gurobi_model.update() # update gurobi model with FFN constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average pooling layer\n",
    "AVG_var = transformer.add_avg_pool(input_var_name:Union[pyo.Var,str], output_var_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connect TNN Output Vars to Problem Definition Vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out: output of last transformer layer\n",
    "# model.out: variable from problem definition that is determined by TNN\n",
    "\n",
    "pyomo_model.out_constraints = pyo.ConstraintList()\n",
    "for i in pyomo_model.out.index_set():\n",
    "    pyomo_model.out_constraints.add(expr= pyomo_model.out[i] == out[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Solve Optimisation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gurobi_model.optimize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyomo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
