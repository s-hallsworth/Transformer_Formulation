{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# following: https://www.geeksforgeeks.org/time-series-forecasting-using-pytorch/\n",
    "\n",
    "# Read data\n",
    "T = 0.5\n",
    "path = f\"..\\\\data\\\\data_T{str(T)}.csv\"\n",
    "names = ['time', 'x1' , 'x2','obs_num']\n",
    "df = pd.read_csv(path,sep=',', header=0, names=names,index_col=False)\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "obs = max(df['obs_num'])\n",
    "print(\"Number of different trajectory functions\",int(obs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "\n",
    "train_data = df[df['time']< (0.7 * T)]\n",
    "test_data = df[df['time']>= (0.7 * T)]\n",
    "print(\"train shape, test shape: \",train_data.shape, test_data.shape)\n",
    "print(train_data.head())\n",
    "\n",
    "train_times = df[df['time']< (0.7 * T)][\"time\"].tolist()\n",
    "test_times =  df[df['time']>= (0.7 * T)][\"time\"].tolist()\n",
    "print(\"train times: \", train_times)\n",
    "print(\"test times: \", test_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1)) # StandardScaler() # \n",
    "\n",
    "# Scaling dataset\n",
    "scaled_train = scaler.fit_transform(train_data.to_numpy())\n",
    "scaled_train = pd.DataFrame(scaled_train, columns=names)\n",
    "print(scaled_train.shape)\n",
    "print(scaled_train.head())\n",
    "\n",
    "scaled_test = scaler.fit_transform(test_data.to_numpy())\n",
    "scaled_test = pd.DataFrame(scaled_test, columns=names)\n",
    "print(scaled_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define positional encoding\n",
    "# def positional_encoding(T, d_model):\n",
    "#     # Initialize positional encoding matrix\n",
    "#     t= np.linspace(0, 1, num=T)\n",
    "#     pe = np.zeros((t.shape[0], d_model))\n",
    "    \n",
    "#     # Compute positional encodings\n",
    "#     for pos, time in enumerate(t):\n",
    "#         for i in range(d_model):\n",
    "#             if i % 2 == 0:\n",
    "#                 pe[pos, i] = np.sin(time / (10000 ** (2 * i / d_model)))\n",
    "#             else:\n",
    "#                 pe[pos, i] = np.cos(time / (10000 ** (2 * (i-1) / d_model)))\n",
    "    \n",
    "#     return np.array(pe)\n",
    "\n",
    "\n",
    "\n",
    "# ## Save data\n",
    "# ID = []\n",
    "# PE = []\n",
    "# d_model  = 1\n",
    "# N = 10\n",
    "\n",
    "# for i in range(N):\n",
    "#     PE += [positional_encoding(T-1, d_model)[:,d_model-1]]\n",
    "\n",
    "# PE = np.array(PE)\n",
    "# print(PE)\n",
    "# PE= PE.flatten()\n",
    "# print(PE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE_train = PE[:training_data_len]\n",
    "# PE_test  = PE[training_data_len:]\n",
    "\n",
    "# scaled_train[\"u\"] = scaled_train[\"u\"] + PE_train\n",
    "# scaled_train[\"dx_dt\"] = scaled_train[\"dx_dt\"] + PE_train\n",
    "\n",
    "# scaled_test[\"u\"] = scaled_test[\"u\"] + PE_test\n",
    "# scaled_test[\"dx_dt\"] = scaled_test[\"dx_dt\"] + PE_test\n",
    "\n",
    "# print(scaled_train.head())\n",
    "# print(scaled_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "def to_sequences(seq_size, df, pred_len):\n",
    "    x = []\n",
    "    y = []\n",
    "    times_x = []\n",
    "    times_y = []\n",
    "    \n",
    "    overlap = 1 #seq_size - pred_len\n",
    "    for obs in df['obs_num'].unique():\n",
    "        df_obs = df[df['obs_num']==obs]\n",
    "        obs_x1 = df_obs['x1'].tolist()\n",
    "        obs_x2 = df_obs['x2'].tolist()\n",
    "        t = df_obs['time'].tolist()\n",
    "        for i in range((len(obs_x1))-seq_size-pred_len) : #create input and pred windows based on trajectory function\n",
    "            \n",
    "            window1 = obs_x1[i:(i+seq_size)]  # x at time t\n",
    "            window2 = obs_x2[i:(i+seq_size)]  # u at time t\n",
    "            t_x = t[i:(i+seq_size)]\n",
    "        \n",
    "            after_window1 = obs_x1[i+seq_size-overlap : i+seq_size+pred_len]  # delta x at time t+1 ( x_t+1 - x_t = delta_x_t+1)\n",
    "            after_window2 = obs_x2[i+seq_size-overlap : i+seq_size+pred_len]\n",
    "            t_y = t[i+seq_size-overlap : i+seq_size+pred_len]\n",
    "            \n",
    "            window = [[x, u] for x,u in zip(window1, window2)]\n",
    "            after_window = [[x, u] for x,u in zip(after_window1, after_window2)]\n",
    "            \n",
    "            x.append(window)\n",
    "            y.append(after_window)\n",
    "            \n",
    "            times_x.append(t_x)\n",
    "            times_y.append(t_y)\n",
    "\n",
    "    #return np.array(x).transpose((0,2,1)), np.array(y).transpose(0,2,1)\n",
    "    return np.array(x), np.array(y), np.array(times_x), np.array(times_y)\n",
    "pred_len = 2\n",
    "sequence_size = 10\n",
    "\n",
    "X_train, y_train, X_train_times, y_train_times = to_sequences(sequence_size, scaled_train, pred_len)\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# Create sequences and labels for testing data\n",
    "X_test,  y_test, X_test_times, y_test_times  = to_sequences(sequence_size, scaled_test, pred_len)\n",
    "# Convert data to PyTorch tensors\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "batch_size = 128\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    ## NB dropout layer is only applied during training not inference (https://keras.io/api/layers/regularization_layers/dropout/)\n",
    "    \n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    # x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    # x = layers.Dropout(dropout)(x)\n",
    "    # x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(inputs.shape[-1], activation=None)(x)\n",
    "    \n",
    "    #x = layers.Conv2D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    #x = layers.Dropout(dropout)(x)\n",
    "    #x = layers.Conv2D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def transformer_decoder(inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Masked multi-head self-attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x, attention_mask=create_look_ahead_mask(x.shape[1]))\n",
    "    x = layers.Dropout(dropout)(attention_output)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Cross-attention with encoder outputs\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, encoder_outputs)\n",
    "    x = layers.Dropout(dropout)(attention_output)\n",
    "    res = x + res\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(inputs.shape[-1])(x)\n",
    "    \n",
    "    return x + res\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    ones = tf.ones((size, size))\n",
    "    mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0) #1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    mask = mask - tf.linalg.diag(tf.ones(size))\n",
    "    print(mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    enc_input_shape,\n",
    "    dec_input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0.25,\n",
    "    mlp_dropout=0.25,\n",
    "):\n",
    "   \n",
    "\n",
    "    inputs = keras.Input(shape=enc_input_shape)\n",
    "    x = inputs\n",
    "    x = layers.Dense(10)(inputs)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    encoder_outputs = x\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_inputs = keras.Input(shape=dec_input_shape)\n",
    "    x = decoder_inputs\n",
    "    x = layers.Dense(10)(decoder_inputs)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_decoder(x, encoder_outputs, head_size, num_heads, ff_dim, dropout)\n",
    "    decoder_outputs = x\n",
    "    \n",
    "    #x = layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x)\n",
    "    #x = layers.GlobalAveragePooling2D(data_format=\"channels_first\")(x)\n",
    "    print(x.shape, x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=None)(x)\n",
    "    return keras.Model(inputs= [inputs, decoder_inputs], outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters:\n",
    "model_params = {\n",
    "    'enc_input_shape': (sequence_size, 2),\n",
    "    'dec_input_shape': (pred_len+1, 2),\n",
    "    'head_size':4,\n",
    "    'num_heads': 1,\n",
    "    'ff_dim': 10, #32\n",
    "    'num_transformer_blocks': 1,\n",
    "    'mlp_units': [24], #24\n",
    "    'mlp_dropout': 0.05,\n",
    "    'dropout': 0.05,\n",
    "    'validation_split': 0.2,\n",
    "    'epochs':2000,\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = build_model(\n",
    "    enc_input_shape = model_params[\"enc_input_shape\"],\n",
    "    dec_input_shape = model_params[\"dec_input_shape\"],\n",
    "    head_size = model_params[\"head_size\"],\n",
    "    num_heads = model_params[\"num_heads\"],\n",
    "    ff_dim = model_params[\"ff_dim\"],\n",
    "    num_transformer_blocks = model_params['num_transformer_blocks'],\n",
    "    mlp_units = model_params[\"mlp_units\"],\n",
    "    mlp_dropout = model_params[\"mlp_dropout\"],\n",
    "    dropout = model_params[\"dropout\"],\n",
    ")\n",
    "\n",
    "# model.compile(\n",
    "#     loss=\"mean_squared_error\",\n",
    "#     optimizer=keras.optimizers.Adam(learning_rate=model_params['learning_rate'])\n",
    "# )\n",
    "# model.summary()\n",
    "\n",
    "# callbacks = [keras.callbacks.EarlyStopping(patience=10, \\\n",
    "#     restore_best_weights=True)]\n",
    "\n",
    "# model.fit(\n",
    "#     [X_train, y_train],\n",
    "#     y_train,\n",
    "#     validation_split = model_params[\"validation_split\"],\n",
    "#     epochs = model_params[\"epochs\"],\n",
    "#     batch_size = model_params[\"batch_size\"],\n",
    "#     callbacks=callbacks,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "\n",
    "# model.evaluate([X_test, y_test], y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "loss = keras.losses.MeanAbsolutePercentageError()\n",
    "optimizer=keras.optimizers.Adam(learning_rate=model_params['learning_rate'])\n",
    "for epochs in range(model_params[\"epochs\"]):\n",
    "    \n",
    "    for src, tgt in train_loader:\n",
    "        output = model() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([X_test[:,:,:], X_test[:,-1:,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'TNN_traj_enc_dec.keras'\n",
    "model.save(name , overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "outputs = []\n",
    "expected = []\n",
    "avg_loss = 0 \n",
    "with torch.no_grad():\n",
    "\n",
    "    for b in range(X_test.shape[0]):   \n",
    "        tgt = X_test[b, -1,:].unsqueeze(0) #overlap\n",
    "        for i in range(pred_len+1):\n",
    "            output = model([X_test[b,:,:], tgt])\n",
    "            # print(i, X_test.shape ,output.shape)\n",
    "            tgt = torch.cat([tgt, output[ -1:, :]], dim=0)\n",
    "\n",
    "        outputs += [output]\n",
    "        expected += [y_test[b,:,:]]\n",
    "        test_loss = criterion(output, y_test[b,:,:])\n",
    "        avg_loss += test_loss.item()\n",
    "    #print(f'Test MSE Loss: {test_loss.item():.4f}')\n",
    "\n",
    "print(\"Average Loss: \", avg_loss/len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.shape)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display train results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colours = [\"r\", \"b\", \"g\", \"m\", \"c\", \"tab:orange\", \"tab:brown\", \"tab:pink\", \"tab:gray\"]\n",
    "data_files = [\"x1\", \"y1\"]\n",
    "for i in [0, 100, 200]: # select batches\n",
    "    for d, name in enumerate(data_files):\n",
    "    \n",
    "        plt.figure(i, figsize=(12, 4))\n",
    "        plt.plot( y_test[i,:,d].detach().numpy(), \"-o\",fillstyle=\"none\", color=colours[d], label=f'{name} expected')\n",
    "        plt.plot( output[i,:,d] , \"o\", color=colours[d], label=f'{name} predicted')\n",
    "        \n",
    "        plt.title(f\"Train Results (simuation {i})\")\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Evaluate the model\n",
    "# model.eval()\n",
    "# outputs = []\n",
    "# with torch.no_grad():\n",
    "#     output = model(X_test, X_test, sequence_size)\n",
    "#     outputs += output\n",
    "#     test_loss = criterion(output, y_test)\n",
    "#     print(f'Test MSE Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "outputs = []\n",
    "expected = []\n",
    "avg_loss = 0 \n",
    "with torch.no_grad():\n",
    "\n",
    "    for b in range(X_test.shape[0]):   \n",
    "        tgt = X_test[b, -1,:].unsqueeze(0) #overlap\n",
    "        for i in range(pred_len+1):\n",
    "            output = model(X_test[b,:,:], tgt, tgt.shape[0])\n",
    "            # print(i, X_test.shape ,output.shape)\n",
    "            tgt = torch.cat([tgt, output[ -1:, :]], dim=0)\n",
    "\n",
    "        outputs += [output]\n",
    "        expected += [targets]\n",
    "        test_loss = criterion(output, y_test[b,:,:])\n",
    "        avg_loss += test_loss.item()\n",
    "    #print(f'Test MSE Loss: {test_loss.item():.4f}')\n",
    "\n",
    "print(\"Average Loss: \", avg_loss/len(outputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save model\n",
    "model_path = \".\\\\toy_pytorch_model_3.pt\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.array(outputs)\n",
    "print(y_test.shape, outputs.shape)\n",
    "print(y_test_times.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display test results\n",
    "import matplotlib.pyplot as plt\n",
    "for i in [0, 100, 250]:\n",
    "    \n",
    "    plt.figure(i, figsize=(6, 4))\n",
    "    plt.plot(X_test_times[i,:], X_test[i,:,0], \"-o\", label='x input')\n",
    "    plt.plot(y_test_times[i,:], y_test[i,:,0], \"-o\", label='x expected')\n",
    "    plt.plot(y_test_times[i,:], outputs[i,:,0] , \"x\", label='x predicted')\n",
    "    \n",
    "    plt.plot(X_test_times[i,:], X_test[i,:,1], \"-o\", label='y input')\n",
    "    plt.plot(y_test_times[i,:],y_test[i,:,1], \"-o\", label='y expected')\n",
    "    plt.plot(y_test_times[i,:],outputs[i,:,1] , \"x\", label='y predicted')\n",
    "    plt.title(f\"Test Results TST on unseen data\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model, input_data=(X_test, X_test, sequence_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformer layers in order of connection:\")\n",
    "for name, layer in model.named_children():\n",
    "    print(f\"{name}: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All submodules in the transformer:\")\n",
    "for name, module in model.named_modules():\n",
    "    print(f\"{name}: {module}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyomo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
