{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       time         u         x   delta_x  obs_num\n",
      "0  0.000000 -0.398684  0.000000  0.000000      1.0\n",
      "1  0.526316 -0.193558 -0.419668 -0.419668      1.0\n",
      "2  1.052632 -0.215139 -0.623413 -0.203745      1.0\n",
      "3  1.578947  0.830190 -0.849875 -0.226462      1.0\n",
      "4  2.105263  0.628167  0.024009  0.873884      1.0\n",
      "(300, 5)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# following: https://www.geeksforgeeks.org/time-series-forecasting-using-pytorch/\n",
    "\n",
    "# Read data\n",
    "T = 10\n",
    "path = f\".\\\\data\\\\data_T{str(T)}_1.csv\"\n",
    "names = ['time', 'u' , 'x','delta_x', 'obs_num']\n",
    "df = pd.read_csv(path,sep=',', header=0, names=names,index_col=False)\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5263157894736842\n",
      "       time         u         x   delta_x  obs_num     dx_dt\n",
      "0  0.000000 -0.398684  0.000000  0.000000      1.0  0.000000\n",
      "1  0.526316 -0.193558 -0.419668 -0.419668      1.0 -0.797369\n",
      "2  1.052632 -0.215139 -0.623413 -0.203745      1.0 -0.387116\n",
      "3  1.578947  0.830190 -0.849875 -0.226462      1.0 -0.430278\n",
      "4  2.105263  0.628167  0.024009  0.873884      1.0  1.660380\n"
     ]
    }
   ],
   "source": [
    "delta_T = df[\"time\"][1] - df[\"time\"][0]\n",
    "print(delta_T)\n",
    "df['dx_dt'] = df[\"delta_x\"]/delta_T\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       time         u         x   delta_x  obs_num     dx_dt\n",
      "1  0.526316 -0.193558 -0.419668 -0.419668      1.0 -0.797369\n",
      "2  1.052632 -0.215139 -0.623413 -0.203745      1.0 -0.387116\n",
      "3  1.578947  0.830190 -0.849875 -0.226462      1.0 -0.430278\n",
      "4  2.105263  0.628167  0.024009  0.873884      1.0  1.660380\n",
      "5  2.631579  0.552348  0.685237  0.661228      1.0  1.256333\n"
     ]
    }
   ],
   "source": [
    "df = df.loc[~((df['time'] == 0))]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n",
      "(228, 6) (57, 6)\n",
      "       time         u         x   delta_x  obs_num     dx_dt\n",
      "1  0.526316 -0.193558 -0.419668 -0.419668      1.0 -0.797369\n",
      "2  1.052632 -0.215139 -0.623413 -0.203745      1.0 -0.387116\n",
      "3  1.578947  0.830190 -0.849875 -0.226462      1.0 -0.430278\n",
      "4  2.105263  0.628167  0.024009  0.873884      1.0  1.660380\n",
      "5  2.631579  0.552348  0.685237  0.661228      1.0  1.256333\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "training_data_len = math.ceil(len(df) * .8)\n",
    "print(training_data_len)\n",
    "\n",
    "# Splitting the dataset\n",
    "train_data = df[:training_data_len]\n",
    "test_data = df[training_data_len:]\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(train_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228, 6)\n",
      "       time         u         x   delta_x  obs_num     dx_dt\n",
      "0  0.000000  0.403056  0.334020  0.300406      0.0  0.300406\n",
      "1  0.055556  0.392256  0.318727  0.403056      0.0  0.403056\n",
      "2  0.111111  0.915359  0.301729  0.392256      0.0  0.392256\n",
      "3  0.166667  0.814263  0.367322  0.915359      0.0  0.915359\n",
      "4  0.222222  0.776322  0.416953  0.814263      0.0  0.814263\n",
      "(57, 6)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1)) # StandardScaler() # \n",
    "names.append(\"dx_dt\")\n",
    "# Scaling dataset\n",
    "scaled_train = scaler.fit_transform(train_data.to_numpy())\n",
    "scaled_train = pd.DataFrame(scaled_train, columns=names)\n",
    "print(scaled_train.shape)\n",
    "print(scaled_train.head())\n",
    "\n",
    "scaled_test = scaler.fit_transform(test_data.to_numpy())\n",
    "scaled_test = pd.DataFrame(scaled_test, columns=names)\n",
    "print(scaled_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define positional encoding\n",
    "# def positional_encoding(T, d_model):\n",
    "#     # Initialize positional encoding matrix\n",
    "#     t= np.linspace(0, 1, num=T)\n",
    "#     pe = np.zeros((t.shape[0], d_model))\n",
    "    \n",
    "#     # Compute positional encodings\n",
    "#     for pos, time in enumerate(t):\n",
    "#         for i in range(d_model):\n",
    "#             if i % 2 == 0:\n",
    "#                 pe[pos, i] = np.sin(time / (10000 ** (2 * i / d_model)))\n",
    "#             else:\n",
    "#                 pe[pos, i] = np.cos(time / (10000 ** (2 * (i-1) / d_model)))\n",
    "    \n",
    "#     return np.array(pe)\n",
    "\n",
    "\n",
    "\n",
    "# ## Save data\n",
    "# ID = []\n",
    "# PE = []\n",
    "# d_model  = 1\n",
    "# N = 10\n",
    "\n",
    "# for i in range(N):\n",
    "#     PE += [positional_encoding(T-1, d_model)[:,d_model-1]]\n",
    "\n",
    "# PE = np.array(PE)\n",
    "# print(PE)\n",
    "# PE= PE.flatten()\n",
    "# print(PE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE_train = PE[:training_data_len]\n",
    "# PE_test  = PE[training_data_len:]\n",
    "\n",
    "# scaled_train[\"u\"] = scaled_train[\"u\"] + PE_train\n",
    "# scaled_train[\"dx_dt\"] = scaled_train[\"dx_dt\"] + PE_train\n",
    "\n",
    "# scaled_test[\"u\"] = scaled_test[\"u\"] + PE_test\n",
    "# scaled_test[\"dx_dt\"] = scaled_test[\"dx_dt\"] + PE_test\n",
    "\n",
    "# print(scaled_train.head())\n",
    "# print(scaled_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([226, 1, 1]) torch.Size([226, 1, 1])\n",
      "tensor([[0.4031]])\n",
      "tensor([[0.4031]])\n",
      "torch.Size([55, 1, 1]) torch.Size([55, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Create sequences\n",
    "def to_sequences(seq_size, df):\n",
    "    obs_u = df['u'].tolist()\n",
    "    obs_x = df['x'].tolist()\n",
    "    obs_delta_x = df['delta_x'].tolist()\n",
    "    obs_dx_dt = df['dx_dt'].tolist()\n",
    "    x = []\n",
    "    y = []\n",
    "    pred_len = 1\n",
    "    \n",
    "    for i in range((len(obs_x))-seq_size - 1):\n",
    "        \n",
    "        # window1 = obs_x[i:(i+seq_size)]  # x at time t\n",
    "        # window2 = obs_u[i:(i+seq_size)]  # u at time t\n",
    "        window2 = [obs_u[i:(i+seq_size)]]  # u at time t\n",
    "        #after_window = [obs_delta_x[i+pred_len: i+seq_size + pred_len]]  # delta x at time t+1 ( x_t+1 - x_t = delta_x_t+1)\n",
    "        #after_window = [obs_dx_dt[i+pred_len: i+seq_size + pred_len]]\n",
    "        after_window = [obs_dx_dt[i+1: i+seq_size+1]]\n",
    "        #window = [[x, u] for x,u in zip(window1, window2)]\n",
    "        \n",
    "        #x.append(window)\n",
    "        x.append(window2)\n",
    "        y.append(after_window)\n",
    "\n",
    "    return np.array(x).transpose((0,2,1)), np.array(y).transpose(0,2,1)\n",
    "    #return np.array(x), np.array(y).transpose(0,2,1)\n",
    "sequence_size = 1\n",
    "\n",
    "X_train, y_train = to_sequences(sequence_size, scaled_train)\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "# Create sequences and labels for testing data\n",
    "X_test,  y_test = to_sequences(sequence_size, scaled_test)\n",
    "# Convert data to PyTorch tensors\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sian_\\OneDrive\\Documents\\Thesis\\MILP_Formulation\\pyomo_env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# # create model\n",
    "tf_model = torch.torch.nn.Transformer(d_model= 1, nhead=1, num_encoder_layers=10, num_decoder_layers=10,dim_feedforward=256, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import TensorDataset, DataLoader \n",
    "\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# # Hyperparameters\n",
    "# seq_length = 10  # Number of time steps to use for predictions\n",
    "# batch_size = 48\n",
    "# input_size = 2  \n",
    "# output_size = 1\n",
    "# hidden_size = 12\n",
    "# num_layers = 4\n",
    "# num_heads = 4\n",
    "# dropout = 0.1\n",
    "# num_epochs = 50\n",
    "# learning_rate = 0.01\n",
    "\n",
    "\n",
    "# # Create dataset and dataloaders\n",
    "# source = X_train\n",
    "# target = y_train\n",
    "# train_dataset = TensorDataset(source, target)\n",
    "# train_loader = DataLoader(train_dataset,  \n",
    "#                         batch_size=batch_size,  \n",
    "#                         shuffle=True)\n",
    "\n",
    "# # Define Transformer Model\n",
    "# class TimeSeriesTransformer(torch.nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout, output_size):\n",
    "#         super(TimeSeriesTransformer, self).__init__()\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.num_heads = num_heads\n",
    "\n",
    "#         self.transformer = torch.nn.Transformer(\n",
    "#             d_model=hidden_size,\n",
    "#             nhead=num_heads,\n",
    "#             num_encoder_layers=num_layers,\n",
    "#             num_decoder_layers=num_layers,\n",
    "#             dropout=dropout,\n",
    "#         )\n",
    "\n",
    "#         self.fc_in = torch.nn.Linear(input_size, hidden_size)\n",
    "#         # self.linear_layer1 = torch.torch.nn.Linear(hidden_size, 128)\n",
    "#         # self.dropout_1     = torch.torch.nn.Dropout(p=dropout)\n",
    "#         self.fc_out = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, src, tgt):\n",
    "#         src = self.fc_in(src)  # Shape: [batch_size, seq_length, hidden_size]\n",
    "#         tgt = self.fc_in(tgt)\n",
    "\n",
    "#         # Transpose for compatibility with torch.nn.Transformer\n",
    "#         src = src.permute(1, 0, 2)  # Shape: [seq_length, batch_size, hidden_size]\n",
    "#         tgt = tgt.permute(1, 0, 2)\n",
    "\n",
    "#         output = self.transformer(src, tgt)  # Shape: [seq_length, batch_size, hidden_size]\n",
    "#         # output = self.linear_layer1(output)\n",
    "#         # output = self.dropout_1(output)\n",
    "#         output = self.fc_out(output)\n",
    "\n",
    "#         return output.permute(1, 0, 2)  # Shape: [batch_size, seq_length, input_size]\n",
    "\n",
    "\n",
    "# # Instantiate model, define loss and optimizer\n",
    "# model = TimeSeriesTransformer(\n",
    "#     input_size=input_size,\n",
    "#     hidden_size=hidden_size,\n",
    "#     num_layers=num_layers,\n",
    "#     num_heads=num_heads,\n",
    "#     dropout=dropout,\n",
    "#     output_size=output_size,\n",
    "# )\n",
    "\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(tf_model.parameters(),lr=0.01)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     train_loss = 0.0\n",
    "\n",
    "#     for src, tgt in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(src[:,:-1,:], src[:, 1:, :])  # We predict the next step, so target is shifted\n",
    "\n",
    "#         loss = criterion(output, tgt[:, 1:, :])\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loss += loss.item() \n",
    "\n",
    "#     train_loss / len(list(train_loader))\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "# print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "\n",
    "# from torch.utils.data import TensorDataset \n",
    "# from torch.utils.data import DataLoader \n",
    "\n",
    "# optimizer = torch.optim.SGD(tf_model.parameters(),lr=0.01)\n",
    "# loss_fn = torch.torch.nn.MSELoss()\n",
    "\n",
    "# def train_epoch(model, optimizer, sequence_size, batch_size=64):\n",
    "#     model.train()\n",
    "#     losses = 0 \n",
    "#     source = X_train\n",
    "#     target = X_train #X_train[1:, :, :]\n",
    "    \n",
    "#     src_mask = tf_model.generate_square_subsequent_mask(sequence_size)\n",
    "#     dataset = TensorDataset(source, target)\n",
    "#     train_dataloader = DataLoader(dataset,  \n",
    "#                         batch_size=batch_size,  \n",
    "#                         shuffle=True)\n",
    "#     for src, tgt in train_dataloader:\n",
    "        \n",
    "#         out = tf_model(src,tgt,src_mask)\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         loss = loss_fn(out,tgt)\n",
    "#         loss.backward()\n",
    "        \n",
    "#         optimizer.step()\n",
    "#         losses += loss.item()\n",
    "#         print(loss.item())\n",
    "#     return losses / len(list(train_dataloader))\n",
    "\n",
    "# def evaluate(model, sequence_size, batch_size=64):\n",
    "#     model.eval()\n",
    "#     losses = 0 \n",
    "#     source = X_test\n",
    "#     target = X_test\n",
    "    \n",
    "#     src_mask = tf_model.generate_square_subsequent_mask(sequence_size)\n",
    "#     dataset = TensorDataset(source, target)\n",
    "#     test_dataloader = DataLoader(dataset,  \n",
    "#                         batch_size=batch_size,  \n",
    "#                         shuffle=True)\n",
    "#     for src, tgt in test_dataloader:\n",
    "#         out = tf_model(src,tgt,src_mask)\n",
    "        \n",
    "#         loss = loss_fn(out,tgt)\n",
    "#         losses += loss.item()\n",
    "#         print(loss.item())\n",
    "#     return losses / len(list(test_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from timeit import default_timer as timer\n",
    "# NUM_EPOCHS = 2\n",
    "\n",
    "# for epoch in range(1, NUM_EPOCHS+1):\n",
    "#     start_time = timer()\n",
    "#     train_loss = train_epoch(tf_model, optimizer, sequence_size, 32)\n",
    "#     end_time = timer()\n",
    "#     print(\"---\")\n",
    "#     val_loss = evaluate(tf_model, sequence_size, 32)\n",
    "#     print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "\n",
    "# from torch.utils.data import TensorDataset \n",
    "# from torch.utils.data import DataLoader \n",
    "\n",
    "# optimizer = torch.optim.Adamax(tf_model.parameters(),lr=0.0001)\n",
    "# loss_fn = torch.torch.nn.MSELoss()\n",
    "\n",
    "# #avg_pool_layer = torch.torch.nn.AvgPool1d(2, stride=1)\n",
    "# linear_layer1 = torch.torch.nn.Linear(1, 128)\n",
    "# dropout_1     = torch.torch.nn.Dropout(p=0.1)\n",
    "# linear_layer2 = torch.torch.nn.Linear(128, 1)\n",
    "\n",
    "# def train_epoch(model, optimizer, sequence_size, batch_size=64):\n",
    "#     model.train()\n",
    "#     losses = 0 \n",
    "#     source = X_train\n",
    "#     target = y_train[:, 1: , :]\n",
    "    \n",
    "#     src_mask = tf_model.generate_square_subsequent_mask(sequence_size)\n",
    "#     dataset = TensorDataset(source, target)\n",
    "#     train_dataloader = DataLoader(dataset,  \n",
    "#                         batch_size=batch_size,  \n",
    "#                         shuffle=False)\n",
    "#     for src, tgt in train_dataloader:\n",
    "#         t_src = src[:, :-1, :]\n",
    "#         t_tgt = tgt\n",
    "#         out = tf_model(t_src, t_tgt,src_mask)\n",
    "#         #out = avg_pool_layer(out)\n",
    "#         out = linear_layer1(out)\n",
    "#         out = dropout_1(out)\n",
    "#         out = linear_layer2(out)\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         loss = loss_fn(out,tgt)\n",
    "#         loss.backward()\n",
    "        \n",
    "#         optimizer.step()\n",
    "#         losses += loss.item()\n",
    "#         # print(loss.item())\n",
    "#     return losses / len(list(train_dataloader))\n",
    "\n",
    "# from timeit import default_timer as timer\n",
    "# NUM_EPOCHS = 400\n",
    "# batch_size = 1000\n",
    "# for epoch in range(1, NUM_EPOCHS+1):\n",
    "#     start_time = timer()\n",
    "#     train_loss = train_epoch(tf_model, optimizer, 9, batch_size)\n",
    "#     end_time = timer()\n",
    "#     print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model=64, nhead=4, num_encoder_layers=3, num_decoder_layers=3):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.encoder = torch.nn.Linear(input_dim, d_model)\n",
    "        self.transformer = torch.nn.Transformer(d_model=d_model, nhead=nhead,\n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers, batch_first=True)\n",
    "        self.decoder = torch.nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.encoder(src)\n",
    "        tgt = self.encoder(tgt)\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = 1  # Since u(t) is 1-dimensional\n",
    "output_dim = 1  # Since x(t) is 1-dimensional\n",
    "model = TransformerModel(input_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.0159\n",
      "Epoch 2/200, Loss: 0.0169\n",
      "Epoch 3/200, Loss: 0.0130\n",
      "Epoch 4/200, Loss: 0.0111\n",
      "Epoch 5/200, Loss: 0.0110\n",
      "Epoch 6/200, Loss: 0.0111\n",
      "Epoch 7/200, Loss: 0.0099\n",
      "Training stopped early, loss is below 0.01.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(inputs, inputs)\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Early stopping if the loss is low enough\n",
    "    if avg_loss <= 0.01:\n",
    "        print(\"Training stopped early, loss is below 0.01.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 0.0057\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    output = model(X_test, X_test)\n",
    "    outputs += output\n",
    "    test_loss = criterion(output, y_test)\n",
    "    print(f'Test MSE Loss: {test_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save model\n",
    "model_path = \".\\\\trained_transformer\\\\toy_pytorch_model.pt\"\n",
    "torch.save(tf_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55, 1, 1]) (55, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "outputs = np.array(outputs)\n",
    "print(X_test.shape, outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a1e18c3880>]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(X_test[:,0,0], \"o\")\n",
    "plt.plot(outputs[:,0,0] , \"x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyomo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
